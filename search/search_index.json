{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stemmer","stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Welcome to FastAPI-Cap!","text":"<p>FastAPI-Cap is a robust and flexible rate-limiting library designed specifically for FastAPI applications. Leveraging the power of Redis and highly optimized Lua scripts, Cap provides a suite of algorithms to effectively control traffic to your APIs, prevent abuse, and ensure stable service delivery.</p> <p>Whether you need simple request limiting or sophisticated traffic shaping, FastAPI-Cap offers a battle-tested solution that integrates seamlessly into your existing FastAPI projects.</p>"},{"location":"#why-fastapi-cap","title":"\ud83c\udf1f Why FastAPI-Cap?","text":"<ul> <li>Diverse Algorithms: Choose from a range of algorithms to perfectly match your     rate-limiting requirements.</li> <li>High Performance: Built on Redis and efficient Lua scripting for atomic     operations and minimal overhead.</li> <li>Easy Integration: Designed as FastAPI dependencies/decorators for a smooth     developer experience.</li> <li>Customizable: Tailor key extraction and rate-limit handling to fit your     application's unique needs.</li> <li>Reliable: Distributed by nature, ensuring consistent limits across multiple     API instances.</li> </ul>"},{"location":"#algorithm-comparison","title":"\u2696\ufe0f Algorithm Comparison","text":"<p>FastAPI-Cap offers several distinct rate-limiting strategies. Each algorithm has its strengths, making it suitable for different use cases. All of these implementations leverage Redis for state management and Lua scripting for atomic, high-performance operations.</p> Algorithm Description Best For Pros Cons Fixed Window Divides time into discrete, fixed windows (e.g., 60 seconds). Requests are counted within the current window, resetting when a new window begins. Simple, aggressive limits; easy to understand. Easy to implement; low memory footprint. Can lead to \"bursts\" at the window boundary if many requests arrive simultaneously right before/after reset. Token Bucket Tokens are added to a \"bucket\" at a constant rate up to a max capacity. Each request consumes one token. Allows for bursts up to bucket capacity, then enforces average rate. Controlling average rate with allowed bursts; smooth traffic flow. Handles bursts well; offers flexibility with capacity; good average throughput. Can allow more requests in short bursts than the average rate if the bucket is full; configuration requires tuning capacity vs. refill rate. Leaky Bucket Requests are placed into a \"bucket\" that has a fixed outflow (leak) rate. If the bucket overflows, new requests are rejected. Ensuring a truly smooth, constant output rate; queueing requests (conceptually). Produces the most consistent output rate; effectively smooths out bursty traffic. Does not directly support bursts (they are absorbed and smoothed); may introduce latency if requests are conceptually queued; configuration can be unintuitive. Generic Cell Rate Algorithm (GCRA) An advanced, precise algorithm that determines a \"Theoretical Arrival Time\" (TAT) for each request. It's similar to Token Bucket but often more precise for burst allowance. Precise burst control; very smooth and fair distribution over time. Highly accurate; excellent for preventing starvation; good for per-user limits. Can be complex to understand and configure initially; <code>retry-after</code> calculation is based on TAT. Approximated Sliding Window Uses two fixed windows (current and previous) and a weighted average to estimate the count in a sliding window. More efficient than log-based, smoother than fixed. Balancing accuracy and resource usage. Better burst handling than fixed window; more memory-efficient than log-based sliding window. Not perfectly accurate (it's an approximation); still susceptible to some boundary issues if not carefully tuned. Sliding Window (Log-based) Records a timestamp for every request in a sorted set. The count is dynamically calculated by removing old timestamps and summing the remaining within the window. Highest accuracy and fairness; eliminates fixed window \"burst\" problem. Most accurate and fair; ensures consistent rate over any time slice within the window. Can be more memory-intensive for high request volumes due to storing individual timestamps."},{"location":"#get-started","title":"\ud83d\ude80 Get Started","text":"<p>Ready to implement robust rate limiting in your FastAPI application?</p> <p>\u27a1\ufe0f Jump to the Quickstart Guide</p> <p>Or explore each algorithm in detail:</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#cap-class","title":"Cap Class","text":""},{"location":"api/#fastapicap.Cap","title":"<code>fastapicap.Cap</code>","text":"<p>Singleton-style Redis connection manager for Cap.</p> <p>This class provides a shared, async Redis connection for all rate limiter instances. It is not meant to be instantiated; use the classmethod <code>init_app</code> to initialize the connection.</p> <p>Attributes:</p> Name Type Description <code>redis</code> <code>Optional[Redis]</code> <p>The shared aioredis Redis connection instance.</p> Example <p>Cap.init_app(\"redis://localhost:6379/0\")</p> Source code in <code>fastapicap/connection.py</code> <pre><code>class Cap:\n    \"\"\"\n    Singleton-style Redis connection manager for Cap.\n\n    This class provides a shared, async Redis connection for all rate limiter\n    instances. It is not meant to be instantiated; use the classmethod\n    `init_app` to initialize the connection.\n\n    Attributes:\n        redis: The shared aioredis Redis connection instance.\n\n    Example:\n        Cap.init_app(\"redis://localhost:6379/0\")\n        # Now Cap.redis can be used by all limiters.\n    \"\"\"\n\n    redis: Optional[Redis] = None\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Prevent instantiation of Cap.\n\n        Raises:\n            RuntimeError: Always, to enforce singleton usage.\n        \"\"\"\n        raise RuntimeError(\"Use classmethods only; do not instantiate Cap.\")\n\n    @classmethod\n    def init_app(cls, redis_url: str) -&gt; None:\n        \"\"\"\n        Initialize the shared Redis connection for Cap.\n\n        Args:\n            redis_url (str): The Redis connection URL.\n\n        Example:\n            Cap.init_app(\"redis://localhost:6379/0\")\n        \"\"\"\n        cls.redis = aioredis.from_url(redis_url, decode_responses=True)\n</code></pre>"},{"location":"api/#fastapicap.Cap--now-capredis-can-be-used-by-all-limiters","title":"Now Cap.redis can be used by all limiters.","text":""},{"location":"api/#fastapicap.Cap.__init__","title":"<code>__init__()</code>","text":"<p>Prevent instantiation of Cap.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Always, to enforce singleton usage.</p> Source code in <code>fastapicap/connection.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Prevent instantiation of Cap.\n\n    Raises:\n        RuntimeError: Always, to enforce singleton usage.\n    \"\"\"\n    raise RuntimeError(\"Use classmethods only; do not instantiate Cap.\")\n</code></pre>"},{"location":"api/#fastapicap.Cap.init_app","title":"<code>init_app(redis_url)</code>  <code>classmethod</code>","text":"<p>Initialize the shared Redis connection for Cap.</p> <p>Parameters:</p> Name Type Description Default <code>redis_url</code> <code>str</code> <p>The Redis connection URL.</p> required Example <p>Cap.init_app(\"redis://localhost:6379/0\")</p> Source code in <code>fastapicap/connection.py</code> <pre><code>@classmethod\ndef init_app(cls, redis_url: str) -&gt; None:\n    \"\"\"\n    Initialize the shared Redis connection for Cap.\n\n    Args:\n        redis_url (str): The Redis connection URL.\n\n    Example:\n        Cap.init_app(\"redis://localhost:6379/0\")\n    \"\"\"\n    cls.redis = aioredis.from_url(redis_url, decode_responses=True)\n</code></pre>"},{"location":"api/#strategies-class","title":"Strategies Class","text":""},{"location":"api/#fastapicap.RateLimiter","title":"<code>fastapicap.RateLimiter</code>","text":"<p>               Bases: <code>BaseLimiter</code></p> <p>Implements a Fixed Window rate limiting algorithm.</p> <p>This limiter restricts the number of requests within a fixed time window. When a new window starts, the counter resets to zero. All requests within the same window consume from the same counter.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of requests allowed within the defined window. Must be a positive integer.</p> required <code>seconds</code> <code>int</code> <p>The number of seconds defining the window size. Can be combined with minutes, hours, or days. Defaults to 0.</p> <code>0</code> <code>minutes</code> <code>int</code> <p>The number of minutes defining the window size. Defaults to 0.</p> <code>0</code> <code>hours</code> <code>int</code> <p>The number of hours defining the window size. Defaults to 0.</p> <code>0</code> <code>days</code> <code>int</code> <p>The number of days defining the window size. Defaults to 0.</p> <code>0</code> <code>key_func</code> <code>Optional[Callable[[Request], str]]</code> <p>An asynchronous or synchronous function to extract a unique key from the request. Defaults to client IP and path.</p> <code>None</code> <code>on_limit</code> <code>Optional[Callable[[Request, Response, int], None]]</code> <p>An asynchronous or synchronous function called when the rate limit is exceeded. Defaults to raising HTTP 429.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Redis key prefix for all limiter keys. Defaults to \"cap\".</p> <code>'cap'</code> <p>Attributes:</p> Name Type Description <code>limit</code> <code>int</code> <p>The maximum requests allowed per window.</p> <code>window_ms</code> <code>int</code> <p>The calculated window size in milliseconds.</p> <code>lua_script</code> <code>str</code> <p>The Lua script used for fixed window logic in Redis.</p> <code>_instance_id</code> <code>str</code> <p>A unique identifier for this limiter instance, used to create distinct Redis keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>limit</code> is not positive or if the calculated <code>window_ms</code> is not positive (i.e., all time units are zero).</p> Source code in <code>fastapicap/strategy/fixed_window.py</code> <pre><code>class RateLimiter(BaseLimiter):\n    \"\"\"\n    Implements a Fixed Window rate limiting algorithm.\n\n    This limiter restricts the number of requests within a fixed time window.\n    When a new window starts, the counter resets to zero. All requests within\n    the same window consume from the same counter.\n\n    Args:\n        limit (int): The maximum number of requests allowed within the defined window.\n            Must be a positive integer.\n        seconds (int): The number of seconds defining the window size.\n            Can be combined with minutes, hours, or days. Defaults to 0.\n        minutes (int): The number of minutes defining the window size.\n            Defaults to 0.\n        hours (int): The number of hours defining the window size.\n            Defaults to 0.\n        days (int): The number of days defining the window size.\n            Defaults to 0.\n        key_func (Optional[Callable[[Request], str]]): An asynchronous or\n            synchronous function to extract a unique key from the request.\n            Defaults to client IP and path.\n        on_limit (Optional[Callable[[Request, Response, int], None]]): An\n            asynchronous or synchronous function called when the rate limit is exceeded.\n            Defaults to raising HTTP 429.\n        prefix (str): Redis key prefix for all limiter keys.\n            Defaults to \"cap\".\n\n    Attributes:\n        limit (int): The maximum requests allowed per window.\n        window_ms (int): The calculated window size in milliseconds.\n        lua_script (str): The Lua script used for fixed window logic in Redis.\n        _instance_id (str): A unique identifier for this limiter instance, used\n            to create distinct Redis keys.\n\n    Raises:\n        ValueError: If the `limit` is not positive or if the calculated\n            `window_ms` is not positive (i.e., all time units are zero).\n    \"\"\"\n\n    def __init__(\n        self,\n        limit: int,\n        seconds: int = 0,\n        minutes: int = 0,\n        hours: int = 0,\n        days: int = 0,\n        key_func: Optional[Callable[[Request], str]] = None,\n        on_limit: Optional[Callable[[Request, Response, int], None]] = None,\n        prefix: str = \"cap\",\n    )-&gt; None:\n        super().__init__(key_func=key_func, on_limit=on_limit, prefix=prefix)\n        self.limit = limit\n        self.window_ms = (\n            (seconds * 1000)\n            + (minutes * 60 * 1000)\n            + (hours * 60 * 60 * 1000)\n            + (days * 24 * 60 * 60 * 1000)\n        )\n        self.lua_script = FIXED_WINDOW\n        self._instance_id: str = f\"fixed_window_limiter_{id(self)}\"\n\n    async def __call__(self, request: Request, response: Response):\n        \"\"\"\n        Apply the rate limiting logic to the incoming request. It interacts with Redis to\n        increment a counter within the current time window and checks if the\n        limit has been exceeded.\n\n        Args:\n            request (Request): The incoming FastAPI request object.\n            response (Response): The FastAPI response object. This can be\n                modified by the `on_limit` handler if needed.\n\n        Raises:\n            HTTPException: By default, if the rate limit is exceeded,\n                `BaseLimiter._default_on_limit` will raise an `HTTPException`\n                with status code 429. Custom `on_limit` functions may raise\n                other exceptions or handle the response differently.\n        \"\"\"\n        redis = self._ensure_redis()\n        await self._ensure_lua_sha(self.lua_script)\n        key: str = await self._safe_call(self.key_func, request)\n        full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n        result = await redis.evalsha(\n            self.lua_sha, 1, full_key, str(self.limit), str(self.window_ms)\n        )\n        allowed = result == 0\n        retry_after = int(result / 1000) if not allowed else 0\n        if not allowed:\n            await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.RateLimiter.__call__","title":"<code>__call__(request, response)</code>  <code>async</code>","text":"<p>Apply the rate limiting logic to the incoming request. It interacts with Redis to increment a counter within the current time window and checks if the limit has been exceeded.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming FastAPI request object.</p> required <code>response</code> <code>Response</code> <p>The FastAPI response object. This can be modified by the <code>on_limit</code> handler if needed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>By default, if the rate limit is exceeded, <code>BaseLimiter._default_on_limit</code> will raise an <code>HTTPException</code> with status code 429. Custom <code>on_limit</code> functions may raise other exceptions or handle the response differently.</p> Source code in <code>fastapicap/strategy/fixed_window.py</code> <pre><code>async def __call__(self, request: Request, response: Response):\n    \"\"\"\n    Apply the rate limiting logic to the incoming request. It interacts with Redis to\n    increment a counter within the current time window and checks if the\n    limit has been exceeded.\n\n    Args:\n        request (Request): The incoming FastAPI request object.\n        response (Response): The FastAPI response object. This can be\n            modified by the `on_limit` handler if needed.\n\n    Raises:\n        HTTPException: By default, if the rate limit is exceeded,\n            `BaseLimiter._default_on_limit` will raise an `HTTPException`\n            with status code 429. Custom `on_limit` functions may raise\n            other exceptions or handle the response differently.\n    \"\"\"\n    redis = self._ensure_redis()\n    await self._ensure_lua_sha(self.lua_script)\n    key: str = await self._safe_call(self.key_func, request)\n    full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n    result = await redis.evalsha(\n        self.lua_sha, 1, full_key, str(self.limit), str(self.window_ms)\n    )\n    allowed = result == 0\n    retry_after = int(result / 1000) if not allowed else 0\n    if not allowed:\n        await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.SlidingWindowRateLimiter","title":"<code>fastapicap.SlidingWindowRateLimiter</code>","text":"<p>               Bases: <code>BaseLimiter</code></p> <p>Implements an Approximated Sliding Window rate limiting algorithm.</p> <p>This algorithm provides a more accurate and smoother rate limiting experience than a simple Fixed Window, while being more memory-efficient than a pure log-based sliding window. It works by maintaining counters for the current fixed window and the immediately preceding fixed window. The effective count for the sliding window is then calculated as a weighted sum of the requests in the previous window and the current window, based on how much of the previous window still \"slides\" into the current view.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of requests allowed within the defined sliding window. Must be a positive integer.</p> required <code>seconds</code> <code>int</code> <p>The number of seconds defining the size of the individual fixed window segments that make up the sliding window. This value, combined with others, determines the <code>window_ms</code>. Defaults to 0.</p> <code>0</code> <code>minutes</code> <code>int</code> <p>The number of minutes defining the window segment size. Defaults to 0.</p> <code>0</code> <code>hours</code> <code>int</code> <p>The number of hours defining the window segment size. Defaults to 0.</p> <code>0</code> <code>days</code> <code>int</code> <p>The number of days defining the window segment size. Defaults to 0.</p> <code>0</code> <code>key_func</code> <code>Optional[Callable[[Request], str]]</code> <p>An asynchronous or synchronous function to extract a unique key from the request. Defaults to client IP and path. The function should accept a <code>fastapi.Request</code> object and return a <code>str</code>.</p> <code>None</code> <code>on_limit</code> <code>Optional[Callable[[Request, Response, int], None]]</code> <p>An asynchronous or synchronous function called when the rate limit is exceeded. Defaults to raising HTTP 429. The function should accept a <code>fastapi.Request</code>, <code>fastapi.Response</code>, and an <code>int</code> (retry_after seconds), and should not return a value.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Redis key prefix for all limiter keys. Defaults to \"cap\".</p> <code>'cap'</code> <p>Attributes:</p> Name Type Description <code>limit</code> <code>int</code> <p>The maximum requests allowed within the sliding window.</p> <code>window_ms</code> <code>int</code> <p>The calculated size of a single fixed window segment in milliseconds (e.g., if you set <code>seconds=60</code>, this is 60000ms). The sliding window itself covers a period equivalent to <code>window_ms</code>.</p> <code>lua_script</code> <code>str</code> <p>The Lua script used for the approximated sliding window logic in Redis.</p> <code>_instance_id</code> <code>str</code> <p>A unique identifier for this limiter instance, used to create distinct Redis keys for isolation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>limit</code> is not positive or if the calculated <code>window_ms</code> is not positive (i.e., all time units are zero).</p> Note <p>This implementation relies on a Redis Lua script to atomically manage and count requests within the current and previous fixed window segments. The <code>retry_after</code> value provided by the Lua script indicates the approximate time in seconds until the next request might be allowed.</p> Source code in <code>fastapicap/strategy/sliding_window.py</code> <pre><code>class SlidingWindowRateLimiter(BaseLimiter):\n    \"\"\"\n    Implements an **Approximated Sliding Window** rate limiting algorithm.\n\n    This algorithm provides a more accurate and smoother rate limiting experience\n    than a simple Fixed Window, while being more memory-efficient than a pure\n    log-based sliding window. It works by maintaining counters for the current\n    fixed window and the immediately preceding fixed window. The effective count\n    for the sliding window is then calculated as a weighted sum of the requests\n    in the previous window and the current window, based on how much of the\n    previous window still \"slides\" into the current view.\n\n    Args:\n        limit (int): The maximum number of requests allowed within the defined\n            sliding window. Must be a positive integer.\n        seconds (int): The number of seconds defining the size of the\n            *individual fixed window segments* that make up the sliding window.\n            This value, combined with others, determines the `window_ms`.\n            Defaults to 0.\n        minutes (int): The number of minutes defining the window segment size.\n            Defaults to 0.\n        hours (int): The number of hours defining the window segment size.\n            Defaults to 0.\n        days (int): The number of days defining the window segment size.\n            Defaults to 0.\n        key_func (Optional[Callable[[Request], str]]): An asynchronous or\n            synchronous function to extract a unique key from the request.\n            Defaults to client IP and path. The function should accept\n            a `fastapi.Request` object and return a `str`.\n        on_limit (Optional[Callable[[Request, Response, int], None]]): An\n            asynchronous or synchronous function called when the rate limit is exceeded.\n            Defaults to raising HTTP 429. The function should accept a\n            `fastapi.Request`, `fastapi.Response`, and an `int` (retry_after seconds),\n            and should not return a value.\n        prefix (str): Redis key prefix for all limiter keys.\n            Defaults to \"cap\".\n\n    Attributes:\n        limit (int): The maximum requests allowed within the sliding window.\n        window_ms (int): The calculated size of a single fixed window segment\n            in milliseconds (e.g., if you set `seconds=60`, this is 60000ms).\n            The sliding window itself covers a period equivalent to `window_ms`.\n        lua_script (str): The Lua script used for the approximated sliding\n            window logic in Redis.\n        _instance_id (str): A unique identifier for this limiter instance, used\n            to create distinct Redis keys for isolation.\n\n    Raises:\n        ValueError: If the `limit` is not positive or if the calculated\n            `window_ms` is not positive (i.e., all time units are zero).\n\n    Note:\n        This implementation relies on a Redis Lua script to atomically manage\n        and count requests within the current and previous fixed window segments.\n        The `retry_after` value provided by the Lua script indicates the\n        approximate time in seconds until the next request might be allowed.\n    \"\"\"\n    def __init__(\n        self,\n        limit: int,\n        seconds: int = 0,\n        minutes: int = 0,\n        hours: int = 0,\n        days: int = 0,\n        key_func: Optional[Callable[[Request], str]] = None,\n        on_limit: Optional[Callable[[Request, Response, int], None]] = None,\n        prefix: str = \"cap\",\n    ):\n        super().__init__(key_func=key_func, on_limit=on_limit, prefix=prefix)\n        self.limit = limit\n        if limit &lt;= 0:\n            raise ValueError(\"Limit must be a positive integer.\")\n        self.window_ms = (\n            (seconds * 1000)\n            + (minutes * 60 * 1000)\n            + (hours * 60 * 60 * 1000)\n            + (days * 24 * 60 * 60 * 1000)\n        )\n        self.lua_script = SLIDING_WINDOW\n        self._instance_id = f\"sliding_window_limiter_{id(self)}\"\n\n    async def __call__(self, request: Request, response: Response):\n        \"\"\"\n        Applies the approximated sliding window rate limiting logic to the incoming request.\n\n        This method is the core of the rate limiter. It interacts with Redis to\n        increment counters for the current and previous window segments and\n        checks if the estimated count within the sliding window exceeds the limit.\n\n        Args:\n            request (Request): The incoming FastAPI request object.\n            response (Response): The FastAPI response object. This can be\n                modified by the `on_limit` handler if needed.\n\n        Raises:\n            HTTPException: By default, if the rate limit is exceeded,\n                `BaseLimiter._default_on_limit` will raise an `HTTPException`\n                with status code 429. Custom `on_limit` functions may raise\n                other exceptions or handle the response differently.\n        \"\"\"\n        redis = self._ensure_redis()\n        await self._ensure_lua_sha(self.lua_script)\n        key: str = await self._safe_call(self.key_func, request)\n        now_ms = int(time.time() * 1000)\n        curr_window_start = now_ms - (now_ms % self.window_ms)\n        prev_window_start = curr_window_start - self.window_ms\n        curr_key = f\"{self.prefix}:{self._instance_id}:{key}:{curr_window_start}\"\n        prev_key = f\"{self.prefix}:{self._instance_id}:{key}:{prev_window_start}\"\n        result = await redis.evalsha(\n            self.lua_sha,\n            2,\n            curr_key,\n            prev_key,\n            str(curr_window_start),\n            str(self.window_ms),\n            str(self.limit),\n        )\n        allowed = result == 0\n        retry_after = int(result / 1000) if not allowed else 0\n        if not allowed:\n            await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.SlidingWindowRateLimiter.__call__","title":"<code>__call__(request, response)</code>  <code>async</code>","text":"<p>Applies the approximated sliding window rate limiting logic to the incoming request.</p> <p>This method is the core of the rate limiter. It interacts with Redis to increment counters for the current and previous window segments and checks if the estimated count within the sliding window exceeds the limit.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming FastAPI request object.</p> required <code>response</code> <code>Response</code> <p>The FastAPI response object. This can be modified by the <code>on_limit</code> handler if needed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>By default, if the rate limit is exceeded, <code>BaseLimiter._default_on_limit</code> will raise an <code>HTTPException</code> with status code 429. Custom <code>on_limit</code> functions may raise other exceptions or handle the response differently.</p> Source code in <code>fastapicap/strategy/sliding_window.py</code> <pre><code>async def __call__(self, request: Request, response: Response):\n    \"\"\"\n    Applies the approximated sliding window rate limiting logic to the incoming request.\n\n    This method is the core of the rate limiter. It interacts with Redis to\n    increment counters for the current and previous window segments and\n    checks if the estimated count within the sliding window exceeds the limit.\n\n    Args:\n        request (Request): The incoming FastAPI request object.\n        response (Response): The FastAPI response object. This can be\n            modified by the `on_limit` handler if needed.\n\n    Raises:\n        HTTPException: By default, if the rate limit is exceeded,\n            `BaseLimiter._default_on_limit` will raise an `HTTPException`\n            with status code 429. Custom `on_limit` functions may raise\n            other exceptions or handle the response differently.\n    \"\"\"\n    redis = self._ensure_redis()\n    await self._ensure_lua_sha(self.lua_script)\n    key: str = await self._safe_call(self.key_func, request)\n    now_ms = int(time.time() * 1000)\n    curr_window_start = now_ms - (now_ms % self.window_ms)\n    prev_window_start = curr_window_start - self.window_ms\n    curr_key = f\"{self.prefix}:{self._instance_id}:{key}:{curr_window_start}\"\n    prev_key = f\"{self.prefix}:{self._instance_id}:{key}:{prev_window_start}\"\n    result = await redis.evalsha(\n        self.lua_sha,\n        2,\n        curr_key,\n        prev_key,\n        str(curr_window_start),\n        str(self.window_ms),\n        str(self.limit),\n    )\n    allowed = result == 0\n    retry_after = int(result / 1000) if not allowed else 0\n    if not allowed:\n        await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.TokenBucketRateLimiter","title":"<code>fastapicap.TokenBucketRateLimiter</code>","text":"<p>               Bases: <code>BaseLimiter</code></p> <p>Implements the Token Bucket rate limiting algorithm.</p> <p>The Token Bucket algorithm works like a bucket that tokens are continuously added to at a fixed <code>refill_rate</code>. Each request consumes one token. If a request arrives and there are tokens available in the bucket, the request is processed, and a token is removed. If the bucket is empty, the request is denied (or queued). The <code>capacity</code> defines the maximum number of tokens the bucket can hold, allowing for bursts of traffic up to that capacity. This algorithm is excellent for controlling the average rate of requests while permitting bursts.</p> <p>Parameters:</p> Name Type Description Default <code>capacity</code> <code>int</code> <p>The maximum number of tokens the bucket can hold. This determines the maximum burst size allowed. Must be a positive integer.</p> required <code>tokens_per_second</code> <code>float</code> <p>The rate at which tokens are added to the bucket, in tokens per second. If combined with other <code>tokens_per_*</code> arguments, they are summed to determine the total <code>refill_rate</code>. Defaults to 0.</p> <code>0</code> <code>tokens_per_minute</code> <code>float</code> <p>The token refill rate in tokens per minute. Defaults to 0.</p> <code>0</code> <code>tokens_per_hour</code> <code>float</code> <p>The token refill rate in tokens per hour. Defaults to 0.</p> <code>0</code> <code>tokens_per_day</code> <code>float</code> <p>The token refill rate in tokens per day. Defaults to 0.</p> <code>0</code> <code>key_func</code> <code>Optional[Callable[[Request], str]]</code> <p>An asynchronous or synchronous function to extract a unique key from the request. Defaults to client IP and path. The function should accept a <code>fastapi.Request</code> object and return a <code>str</code>.</p> <code>None</code> <code>on_limit</code> <code>Optional[Callable[[Request, Response, int], None]]</code> <p>An asynchronous or synchronous function called when the rate limit is exceeded. Defaults to raising HTTP 429. The function should accept a <code>fastapi.Request</code>, <code>fastapi.Response</code>, and an <code>int</code> (retry_after seconds), and should not return a value.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Redis key prefix for all limiter keys. Defaults to \"cap\".</p> <code>'cap'</code> <p>Attributes:</p> Name Type Description <code>capacity</code> <code>int</code> <p>The configured maximum bucket capacity.</p> <code>refill_rate</code> <code>float</code> <p>The total calculated token refill rate in tokens per millisecond.</p> <code>lua_script</code> <code>str</code> <p>The Lua script used for token bucket logic in Redis.</p> <code>_instance_id</code> <code>str</code> <p>A unique identifier for this limiter instance, used to create distinct Redis keys for isolation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>capacity</code> is not positive, or if the total calculated <code>refill_rate</code> is not positive. This ensures a valid configuration for the token bucket.</p> Source code in <code>fastapicap/strategy/token_bucket.py</code> <pre><code>class TokenBucketRateLimiter(BaseLimiter):\n    \"\"\"\n    Implements the Token Bucket rate limiting algorithm.\n\n    The Token Bucket algorithm works like a bucket that tokens are continuously\n    added to at a fixed `refill_rate`. Each request consumes one token.\n    If a request arrives and there are tokens available in the bucket,\n    the request is processed, and a token is removed. If the bucket is empty,\n    the request is denied (or queued). The `capacity` defines the maximum\n    number of tokens the bucket can hold, allowing for bursts of traffic\n    up to that capacity. This algorithm is excellent for controlling the\n    average rate of requests while permitting bursts.\n\n    Args:\n        capacity (int): The maximum number of tokens the bucket can hold.\n            This determines the maximum burst size allowed. Must be a\n            positive integer.\n        tokens_per_second (float): The rate at which tokens are added to the\n            bucket, in tokens per second. If combined with other `tokens_per_*`\n            arguments, they are summed to determine the total `refill_rate`.\n            Defaults to 0.\n        tokens_per_minute (float): The token refill rate in tokens per minute.\n            Defaults to 0.\n        tokens_per_hour (float): The token refill rate in tokens per hour.\n            Defaults to 0.\n        tokens_per_day (float): The token refill rate in tokens per day.\n            Defaults to 0.\n        key_func (Optional[Callable[[Request], str]]): An asynchronous or\n            synchronous function to extract a unique key from the request.\n            Defaults to client IP and path. The function should accept\n            a `fastapi.Request` object and return a `str`.\n        on_limit (Optional[Callable[[Request, Response, int], None]]): An\n            asynchronous or synchronous function called when the rate limit is exceeded.\n            Defaults to raising HTTP 429. The function should accept a\n            `fastapi.Request`, `fastapi.Response`, and an `int` (retry_after seconds),\n            and should not return a value.\n        prefix (str): Redis key prefix for all limiter keys.\n            Defaults to \"cap\".\n\n    Attributes:\n        capacity (int): The configured maximum bucket capacity.\n        refill_rate (float): The total calculated token refill rate in\n            tokens per millisecond.\n        lua_script (str): The Lua script used for token bucket logic in Redis.\n        _instance_id (str): A unique identifier for this limiter instance, used\n            to create distinct Redis keys for isolation.\n\n    Raises:\n        ValueError: If the `capacity` is not positive, or if the total\n            calculated `refill_rate` is not positive. This ensures a valid\n            configuration for the token bucket.\n    \"\"\"\n    def __init__(\n        self,\n        capacity: int,\n        tokens_per_second: float = 0,\n        tokens_per_minute: float = 0,\n        tokens_per_hour: float = 0,\n        tokens_per_day: float = 0,\n        key_func: Optional[Callable[[Request], str]] = None,\n        on_limit: Optional[Callable[[Request, Response, int], None]] = None,\n        prefix: str = \"cap\",\n    ):\n        super().__init__(key_func=key_func, on_limit=on_limit, prefix=prefix)\n        if capacity &lt;= 0:\n            raise ValueError(\"Capacity must be a positive integer.\")\n\n        self.capacity = capacity\n        total_tokens = (\n            tokens_per_second\n            + tokens_per_minute / 60\n            + tokens_per_hour / 3600\n            + tokens_per_day / 86400\n        )\n        self.refill_rate = total_tokens / 1000\n        self.lua_script = TOKEN_BUCKET\n        self._instance_id = f\"token_bucket_limiter_{id(self)}\"\n\n        if self.refill_rate &lt;= 0:\n            raise ValueError(\n                \"Refill rate must be positive.\"\n                \"Check your tokens_per_second/minute/hour/day arguments.\"\n            )\n\n    async def __call__(self, request: Request, response: Response):\n        \"\"\"\n        Applies the Token Bucket rate limiting logic to the incoming request.\n\n        This method is the core of the rate limiter. It interacts with Redis to simulate\n        token consumption and bucket refill, determining if the request is allowed.\n\n        Args:\n            request (Request): The incoming FastAPI request object.\n            response (Response): The FastAPI response object. This can be\n                modified by the `on_limit` handler if needed.\n\n        Raises:\n            HTTPException: By default, if the rate limit is exceeded,\n                `BaseLimiter._default_on_limit` will raise an `HTTPException`\n                with status code 429. Custom `on_limit` functions may raise\n                other exceptions or handle the response differently.\n        \"\"\"\n        redis = self._ensure_redis()\n        await self._ensure_lua_sha(self.lua_script)\n        key: str = await self._safe_call(self.key_func, request)\n        full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n        now = int(time.time() * 1000)\n        result = await redis.evalsha(\n            self.lua_sha,\n            1,\n            full_key,\n            str(self.capacity),\n            str(self.refill_rate),\n            str(now),\n        )\n        allowed = result == 0\n        retry_after = int(result) // 1000 if not allowed else 0\n        if not allowed:\n            await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.TokenBucketRateLimiter.__call__","title":"<code>__call__(request, response)</code>  <code>async</code>","text":"<p>Applies the Token Bucket rate limiting logic to the incoming request.</p> <p>This method is the core of the rate limiter. It interacts with Redis to simulate token consumption and bucket refill, determining if the request is allowed.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming FastAPI request object.</p> required <code>response</code> <code>Response</code> <p>The FastAPI response object. This can be modified by the <code>on_limit</code> handler if needed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>By default, if the rate limit is exceeded, <code>BaseLimiter._default_on_limit</code> will raise an <code>HTTPException</code> with status code 429. Custom <code>on_limit</code> functions may raise other exceptions or handle the response differently.</p> Source code in <code>fastapicap/strategy/token_bucket.py</code> <pre><code>async def __call__(self, request: Request, response: Response):\n    \"\"\"\n    Applies the Token Bucket rate limiting logic to the incoming request.\n\n    This method is the core of the rate limiter. It interacts with Redis to simulate\n    token consumption and bucket refill, determining if the request is allowed.\n\n    Args:\n        request (Request): The incoming FastAPI request object.\n        response (Response): The FastAPI response object. This can be\n            modified by the `on_limit` handler if needed.\n\n    Raises:\n        HTTPException: By default, if the rate limit is exceeded,\n            `BaseLimiter._default_on_limit` will raise an `HTTPException`\n            with status code 429. Custom `on_limit` functions may raise\n            other exceptions or handle the response differently.\n    \"\"\"\n    redis = self._ensure_redis()\n    await self._ensure_lua_sha(self.lua_script)\n    key: str = await self._safe_call(self.key_func, request)\n    full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n    now = int(time.time() * 1000)\n    result = await redis.evalsha(\n        self.lua_sha,\n        1,\n        full_key,\n        str(self.capacity),\n        str(self.refill_rate),\n        str(now),\n    )\n    allowed = result == 0\n    retry_after = int(result) // 1000 if not allowed else 0\n    if not allowed:\n        await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.LeakyBucketRateLimiter","title":"<code>fastapicap.LeakyBucketRateLimiter</code>","text":"<p>               Bases: <code>BaseLimiter</code></p> <p>Implements the Leaky Bucket rate limiting algorithm.</p> <p>The Leaky Bucket algorithm models traffic flow like water in a bucket. Requests are \"drops\" added to the bucket. If the bucket overflows (exceeds capacity), new requests are rejected. \"Water\" (requests) leaks out of the bucket at a constant rate, making space for new requests. This algorithm is known for producing a smooth, constant output rate of requests, which helps in preventing bursts from overwhelming downstream services.</p> <p>Parameters:</p> Name Type Description Default <code>capacity</code> <code>int</code> <p>The maximum capacity of the bucket. This represents the maximum number of requests that can be held in the bucket before new requests are rejected. Must be a positive integer.</p> required <code>leaks_per_second</code> <code>float</code> <p>The rate at which requests \"leak\" (are processed) from the bucket, in requests per second. If combined with other <code>leaks_per_*</code> arguments, they are summed. Defaults to 0.</p> <code>0</code> <code>leaks_per_minute</code> <code>float</code> <p>The leak rate in requests per minute. Defaults to 0.</p> <code>0</code> <code>leaks_per_hour</code> <code>float</code> <p>The leak rate in requests per hour. Defaults to 0.</p> <code>0</code> <code>leaks_per_day</code> <code>float</code> <p>The leak rate in requests per day. Defaults to 0.</p> <code>0</code> <code>key_func</code> <code>Optional[Callable[[Request], str]]</code> <p>An asynchronous or synchronous function to extract a unique key from the request. Defaults to client IP and path. The function should accept a <code>fastapi.Request</code> object and return a <code>str</code>.</p> <code>None</code> <code>on_limit</code> <code>Optional[Callable[[Request, Response, int], None]]</code> <p>An asynchronous or synchronous function called when the rate limit is exceeded. Defaults to raising HTTP 429. The function should accept a <code>fastapi.Request</code>, <code>fastapi.Response</code>, and an <code>int</code> (retry_after seconds), and should not return a value.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Redis key prefix for all limiter keys. Defaults to \"cap\".</p> <code>'cap'</code> <p>Attributes:</p> Name Type Description <code>capacity</code> <code>int</code> <p>The configured maximum bucket capacity.</p> <code>leak_rate</code> <code>float</code> <p>The total calculated leak rate in requests per millisecond.</p> <code>lua_script</code> <code>str</code> <p>The Lua script used for leaky bucket logic in Redis.</p> <code>_instance_id</code> <code>str</code> <p>A unique identifier for this limiter instance, used to create distinct Redis keys for isolation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>capacity</code> is not positive, or if the total calculated <code>leak_rate</code> is not positive. This ensures a valid configuration for the leaky bucket.</p> Source code in <code>fastapicap/strategy/leaky_bucket.py</code> <pre><code>class LeakyBucketRateLimiter(BaseLimiter):\n    \"\"\"\n    Implements the Leaky Bucket rate limiting algorithm.\n\n    The Leaky Bucket algorithm models traffic flow like water in a bucket.\n    Requests are \"drops\" added to the bucket. If the bucket overflows (exceeds capacity),\n    new requests are rejected. \"Water\" (requests) leaks out of the bucket at a\n    constant rate, making space for new requests. This algorithm is known for\n    producing a smooth, constant output rate of requests, which helps in\n    preventing bursts from overwhelming downstream services.\n\n    Args:\n        capacity (int): The maximum capacity of the bucket. This represents\n            the maximum number of requests that can be held in the bucket\n            before new requests are rejected. Must be a positive integer.\n        leaks_per_second (float): The rate at which requests \"leak\" (are processed)\n            from the bucket, in requests per second. If combined with other\n            `leaks_per_*` arguments, they are summed. Defaults to 0.\n        leaks_per_minute (float): The leak rate in requests per minute.\n            Defaults to 0.\n        leaks_per_hour (float): The leak rate in requests per hour.\n            Defaults to 0.\n        leaks_per_day (float): The leak rate in requests per day.\n            Defaults to 0.\n        key_func (Optional[Callable[[Request], str]]): An asynchronous or\n            synchronous function to extract a unique key from the request.\n            Defaults to client IP and path. The function should accept\n            a `fastapi.Request` object and return a `str`.\n        on_limit (Optional[Callable[[Request, Response, int], None]]): An\n            asynchronous or synchronous function called when the rate limit is exceeded.\n            Defaults to raising HTTP 429. The function should accept a\n            `fastapi.Request`, `fastapi.Response`, and an `int` (retry_after seconds),\n            and should not return a value.\n        prefix (str): Redis key prefix for all limiter keys.\n            Defaults to \"cap\".\n\n    Attributes:\n        capacity (int): The configured maximum bucket capacity.\n        leak_rate (float): The total calculated leak rate in requests per millisecond.\n        lua_script (str): The Lua script used for leaky bucket logic in Redis.\n        _instance_id (str): A unique identifier for this limiter instance, used\n            to create distinct Redis keys for isolation.\n\n    Raises:\n        ValueError: If the `capacity` is not positive, or if the total\n            calculated `leak_rate` is not positive. This ensures a valid\n            configuration for the leaky bucket.\n    \"\"\"\n    def __init__(\n        self,\n        capacity: int,\n        leaks_per_second: float = 0,\n        leaks_per_minute: float = 0,\n        leaks_per_hour: float = 0,\n        leaks_per_day: float = 0,\n        key_func: Optional[Callable[[Request], str]] = None,\n        on_limit: Optional[Callable[[Request, Response, int], None]] = None,\n        prefix: str = \"cap\",\n    ):\n        super().__init__(key_func=key_func, on_limit=on_limit, prefix=prefix)\n        self.capacity = capacity\n        if capacity &lt;= 0:\n            raise ValueError(\"Capacity must be a positive integer.\")\n        total_leaks = (\n            leaks_per_second\n            + leaks_per_minute / 60\n            + leaks_per_hour / 3600\n            + leaks_per_day / 86400\n        )\n        self.leak_rate = total_leaks / 1000\n        self.lua_script = LEAKY_BUCKET\n        self._instance_id = f\"leaky_bucket_limiter_{id(self)}\"\n\n    async def __call__(self, request: Request, response: Response):\n        \"\"\"\n        Applies the leaky bucket rate limiting logic to the incoming request.\n\n        This method is the core of the rate limiter. It interacts with Redis to simulate\n        adding a \"drop\" to the bucket and checks if it overflows.\n\n        Args:\n            request (Request): The incoming FastAPI request object.\n            response (Response): The FastAPI response object. This can be\n                modified by the `on_limit` handler if needed.\n\n        Raises:\n            HTTPException: By default, if the rate limit is exceeded,\n                `BaseLimiter._default_on_limit` will raise an `HTTPException`\n                with status code 429. Custom `on_limit` functions may raise\n                other exceptions or handle the response differently.\n        \"\"\"\n        redis = self._ensure_redis()\n        await self._ensure_lua_sha(self.lua_script)\n        key: str = await self._safe_call(self.key_func, request)\n        full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n        now = int(time.time() * 1000)\n        result = await redis.evalsha(\n            self.lua_sha,\n            1,\n            full_key,\n            str(self.capacity),\n            str(self.leak_rate),\n            str(now),\n        )\n        allowed = result == 0\n        retry_after = (int(result) + 999) // 1000 if not allowed else 0\n        if not allowed:\n            await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.LeakyBucketRateLimiter.__call__","title":"<code>__call__(request, response)</code>  <code>async</code>","text":"<p>Applies the leaky bucket rate limiting logic to the incoming request.</p> <p>This method is the core of the rate limiter. It interacts with Redis to simulate adding a \"drop\" to the bucket and checks if it overflows.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming FastAPI request object.</p> required <code>response</code> <code>Response</code> <p>The FastAPI response object. This can be modified by the <code>on_limit</code> handler if needed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>By default, if the rate limit is exceeded, <code>BaseLimiter._default_on_limit</code> will raise an <code>HTTPException</code> with status code 429. Custom <code>on_limit</code> functions may raise other exceptions or handle the response differently.</p> Source code in <code>fastapicap/strategy/leaky_bucket.py</code> <pre><code>async def __call__(self, request: Request, response: Response):\n    \"\"\"\n    Applies the leaky bucket rate limiting logic to the incoming request.\n\n    This method is the core of the rate limiter. It interacts with Redis to simulate\n    adding a \"drop\" to the bucket and checks if it overflows.\n\n    Args:\n        request (Request): The incoming FastAPI request object.\n        response (Response): The FastAPI response object. This can be\n            modified by the `on_limit` handler if needed.\n\n    Raises:\n        HTTPException: By default, if the rate limit is exceeded,\n            `BaseLimiter._default_on_limit` will raise an `HTTPException`\n            with status code 429. Custom `on_limit` functions may raise\n            other exceptions or handle the response differently.\n    \"\"\"\n    redis = self._ensure_redis()\n    await self._ensure_lua_sha(self.lua_script)\n    key: str = await self._safe_call(self.key_func, request)\n    full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n    now = int(time.time() * 1000)\n    result = await redis.evalsha(\n        self.lua_sha,\n        1,\n        full_key,\n        str(self.capacity),\n        str(self.leak_rate),\n        str(now),\n    )\n    allowed = result == 0\n    retry_after = (int(result) + 999) // 1000 if not allowed else 0\n    if not allowed:\n        await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.GCRARateLimiter","title":"<code>fastapicap.GCRARateLimiter</code>","text":"<p>               Bases: <code>BaseLimiter</code></p> <p>Implements the Generic Cell Rate Algorithm (GCRA) for rate limiting.</p> <p>GCRA is a popular algorithm that controls the rate of events by tracking the \"Theoretical Arrival Time\" (TAT) of the next allowed event. It's often used for API rate limiting as it provides a smooth, burstable rate.</p> <p>This limiter allows for a burst of requests up to <code>burst</code> capacity, and then enforces a steady rate defined by <code>tokens_per_second</code>, <code>tokens_per_minute</code>, <code>tokens_per_hour</code>, or <code>tokens_per_day</code>.</p> <p>Parameters:</p> Name Type Description Default <code>burst</code> <code>int</code> <p>The maximum number of additional requests that can be served instantly (i.e., the \"burst\" capacity beyond the steady rate). This defines how many requests can be handled without delay if the system has been idle.</p> required <code>tokens_per_second</code> <code>float</code> <p>The steady rate of tokens allowed per second. If combined with other <code>tokens_per_*</code> arguments, they are summed. Defaults to 0.</p> <code>0</code> <code>tokens_per_minute</code> <code>float</code> <p>The steady rate of tokens allowed per minute. Defaults to 0.</p> <code>0</code> <code>tokens_per_hour</code> <code>float</code> <p>The steady rate of tokens allowed per hour. Defaults to 0.</p> <code>0</code> <code>tokens_per_day</code> <code>float</code> <p>The steady rate of tokens allowed per day. Defaults to 0.</p> <code>0</code> <code>key_func</code> <code>Optional[Callable[[Request], str]]</code> <p>An asynchronous function to extract a unique key from the request. This key is used to identify the subject being rate-limited (e.g., client IP, user ID). If <code>None</code>, <code>BaseLimiter._default_key_func</code> (client IP + path) is used.</p> <code>None</code> <code>on_limit</code> <code>Optional[Callable[[Request, Response, int], None]]</code> <p>An asynchronous function called when the rate limit is exceeded. It receives the <code>request</code>, <code>response</code> object, and the <code>retry_after</code> value in seconds. If <code>None</code>, <code>BaseLimiter._default_on_limit</code> (which raises an <code>HTTPException 429</code>) is used.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>A string prefix for all Redis keys used by this limiter. Defaults to \"cap\".</p> <code>'cap'</code> <p>Attributes:</p> Name Type Description <code>burst</code> <code>int</code> <p>The configured burst capacity.</p> <code>tokens_per_second</code> <code>float</code> <p>The total calculated steady rate in tokens per second.</p> <code>period</code> <code>float</code> <p>The calculated time period (in milliseconds) between allowed tokens.</p> <code>lua_script</code> <code>str</code> <p>The Lua script used for GCRA logic in Redis.</p> <code>_instance_id</code> <code>str</code> <p>A unique identifier for this limiter instance, used to create distinct Redis keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the total calculated <code>tokens_per_second</code> is not positive. This ensures that a meaningful rate limit is defined.</p> Note <p>The <code>GCRA_LUA</code> script handles the core rate-limiting logic in Redis, ensuring atomic operations. The <code>retry_after</code> value returned by the Lua script (if a limit is hit) indicates the number of milliseconds until the next request would be allowed.</p> Source code in <code>fastapicap/strategy/gcra.py</code> <pre><code>class GCRARateLimiter(BaseLimiter):\n    \"\"\"\n    Implements the Generic Cell Rate Algorithm (GCRA) for rate limiting.\n\n    GCRA is a popular algorithm that controls the rate of events by tracking\n    the \"Theoretical Arrival Time\" (TAT) of the next allowed event. It's often\n    used for API rate limiting as it provides a smooth, burstable rate.\n\n    This limiter allows for a burst of requests up to `burst` capacity,\n    and then enforces a steady rate defined by `tokens_per_second`,\n    `tokens_per_minute`, `tokens_per_hour`, or `tokens_per_day`.\n\n    Args:\n        burst (int): The maximum number of additional requests that can be\n            served instantly (i.e., the \"burst\" capacity beyond the steady rate).\n            This defines how many requests can be handled without delay if\n            the system has been idle.\n        tokens_per_second (float): The steady rate of tokens allowed per second.\n            If combined with other `tokens_per_*` arguments, they are summed.\n            Defaults to 0.\n        tokens_per_minute (float): The steady rate of tokens allowed per minute.\n            Defaults to 0.\n        tokens_per_hour (float): The steady rate of tokens allowed per hour.\n            Defaults to 0.\n        tokens_per_day (float): The steady rate of tokens allowed per day.\n            Defaults to 0.\n        key_func (Optional[Callable[[Request], str]]): An asynchronous function\n            to extract a unique key from the request. This key is used to\n            identify the subject being rate-limited (e.g., client IP, user ID).\n            If `None`, `BaseLimiter._default_key_func` (client IP + path) is used.\n        on_limit (Optional[Callable[[Request, Response, int], None]]): An\n            asynchronous function called when the rate limit is exceeded.\n            It receives the `request`, `response` object, and the `retry_after`\n            value in seconds. If `None`, `BaseLimiter._default_on_limit`\n            (which raises an `HTTPException 429`) is used.\n        prefix (str): A string prefix for all Redis keys used by this limiter.\n            Defaults to \"cap\".\n\n    Attributes:\n        burst (int): The configured burst capacity.\n        tokens_per_second (float): The total calculated steady rate in tokens per second.\n        period (float): The calculated time period (in milliseconds) between allowed tokens.\n        lua_script (str): The Lua script used for GCRA logic in Redis.\n        _instance_id (str): A unique identifier for this limiter instance, used\n            to create distinct Redis keys.\n\n    Raises:\n        ValueError: If the total calculated `tokens_per_second` is not positive.\n            This ensures that a meaningful rate limit is defined.\n\n    Note:\n        The `GCRA_LUA` script handles the core rate-limiting logic in Redis,\n        ensuring atomic operations. The `retry_after` value returned by the\n        Lua script (if a limit is hit) indicates the number of milliseconds\n        until the next request would be allowed.\n    \"\"\"\n\n    def __init__(\n        self,\n        burst: int,\n        tokens_per_second: float = 0,\n        tokens_per_minute: float = 0,\n        tokens_per_hour: float = 0,\n        tokens_per_day: float = 0,\n        key_func: Optional[Callable[[Request], str]] = None,\n        on_limit: Optional[Callable[[Request, Response, int], None]] = None,\n        prefix: str = \"cap\",\n    ):\n        super().__init__(key_func=key_func, on_limit=on_limit, prefix=prefix)\n        self.burst = burst\n        total_tokens_per_second = (\n            tokens_per_second\n            + tokens_per_minute / 60\n            + tokens_per_hour / 3600\n            + tokens_per_day / 86400\n        )\n        if total_tokens_per_second &lt;= 0:\n            raise ValueError(\n                \"At least one of tokens_per_second, tokens_per_minute, \"\n                \"tokens_per_hour, or tokens_per_day must be positive.\"\n            )\n\n        self.tokens_per_second = total_tokens_per_second\n        self.period = 1000.0 / self.tokens_per_second\n        self.lua_script = GCRA_LUA\n        self._instance_id = f\"gcra_{id(self)}\"\n\n    async def __call__(self, request: Request, response: Response):\n        \"\"\"\n        Executes the GCRA rate-limiting logic for the incoming request.\n\n        This method is designed to be used as a FastAPI dependency or decorator.\n        It interacts with Redis to check if the request is allowed based on\n        the configured GCRA parameters. If the limit is exceeded, it calls\n        the `on_limit` handler.\n\n        Args:\n            request (Request): The incoming FastAPI request object.\n            response (Response): The FastAPI response object. This can be\n                modified by the `on_limit` handler if needed.\n\n        Raises:\n            HTTPException: By default, if the rate limit is exceeded,\n                `BaseLimiter._default_on_limit` will raise an `HTTPException`\n                with status code 429. Custom `on_limit` functions may raise\n                other exceptions or handle the response differently.\n        \"\"\"\n        redis = self._ensure_redis()\n        await self._ensure_lua_sha(self.lua_script)\n        key: str = await self._safe_call(self.key_func, request)\n        full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n        now = int(time.time() * 1000)\n        result = await redis.evalsha(\n            self.lua_sha,\n            1,\n            full_key,\n            str(self.burst),\n            str(self.tokens_per_second / 1000),  # tokens/ms\n            str(self.period),\n            str(now),\n        )\n        allowed = result[0] == 1\n        retry_after = int(result[1]) if not allowed else 0\n        if not allowed:\n            await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.GCRARateLimiter.__call__","title":"<code>__call__(request, response)</code>  <code>async</code>","text":"<p>Executes the GCRA rate-limiting logic for the incoming request.</p> <p>This method is designed to be used as a FastAPI dependency or decorator. It interacts with Redis to check if the request is allowed based on the configured GCRA parameters. If the limit is exceeded, it calls the <code>on_limit</code> handler.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming FastAPI request object.</p> required <code>response</code> <code>Response</code> <p>The FastAPI response object. This can be modified by the <code>on_limit</code> handler if needed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>By default, if the rate limit is exceeded, <code>BaseLimiter._default_on_limit</code> will raise an <code>HTTPException</code> with status code 429. Custom <code>on_limit</code> functions may raise other exceptions or handle the response differently.</p> Source code in <code>fastapicap/strategy/gcra.py</code> <pre><code>async def __call__(self, request: Request, response: Response):\n    \"\"\"\n    Executes the GCRA rate-limiting logic for the incoming request.\n\n    This method is designed to be used as a FastAPI dependency or decorator.\n    It interacts with Redis to check if the request is allowed based on\n    the configured GCRA parameters. If the limit is exceeded, it calls\n    the `on_limit` handler.\n\n    Args:\n        request (Request): The incoming FastAPI request object.\n        response (Response): The FastAPI response object. This can be\n            modified by the `on_limit` handler if needed.\n\n    Raises:\n        HTTPException: By default, if the rate limit is exceeded,\n            `BaseLimiter._default_on_limit` will raise an `HTTPException`\n            with status code 429. Custom `on_limit` functions may raise\n            other exceptions or handle the response differently.\n    \"\"\"\n    redis = self._ensure_redis()\n    await self._ensure_lua_sha(self.lua_script)\n    key: str = await self._safe_call(self.key_func, request)\n    full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n    now = int(time.time() * 1000)\n    result = await redis.evalsha(\n        self.lua_sha,\n        1,\n        full_key,\n        str(self.burst),\n        str(self.tokens_per_second / 1000),  # tokens/ms\n        str(self.period),\n        str(now),\n    )\n    allowed = result[0] == 1\n    retry_after = int(result[1]) if not allowed else 0\n    if not allowed:\n        await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.SlidingWindowLogRateLimiter","title":"<code>fastapicap.SlidingWindowLogRateLimiter</code>","text":"<p>               Bases: <code>BaseLimiter</code></p> <p>Implements a Sliding Window (Log-based) rate limiting algorithm.</p> <p>This is the most accurate form of the sliding window algorithm. It works by storing a timestamp for every request made by a client within a Redis sorted set. When a new request comes in, the algorithm first removes all timestamps that fall outside the current sliding window. Then, it counts the number of remaining timestamps within the window. If this count is below the <code>limit</code>, the request is allowed, and its timestamp is added to the set. This method ensures precise rate limiting as the window truly \"slides\" over time.</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>The maximum number of requests allowed within the defined sliding window. Must be a positive integer.</p> required <code>window_seconds</code> <code>int</code> <p>The number of seconds defining the size of the sliding window. Can be combined with minutes, hours, or days. Defaults to 0.</p> <code>0</code> <code>window_minutes</code> <code>int</code> <p>The number of minutes defining the window size. Defaults to 0.</p> <code>0</code> <code>window_hours</code> <code>int</code> <p>The number of hours defining the window size. Defaults to 0.</p> <code>0</code> <code>window_days</code> <code>int</code> <p>The number of days defining the window size. Defaults to 0.</p> <code>0</code> <code>key_func</code> <code>Optional[Callable[[Request], str]]</code> <p>An asynchronous or synchronous function to extract a unique key from the request. Defaults to client IP and path. The function should accept a <code>fastapi.Request</code> object and return a <code>str</code>.</p> <code>None</code> <code>on_limit</code> <code>Optional[Callable[[Request, Response, int], None]]</code> <p>An asynchronous or synchronous function called when the rate limit is exceeded. Defaults to raising HTTP 429. The function should accept a <code>fastapi.Request</code>, <code>fastapi.Response</code>, and an <code>int</code> (retry_after seconds), and should not return a value.</p> <code>None</code> <code>prefix</code> <code>str</code> <p>Redis key prefix for all limiter keys. Defaults to \"cap\".</p> <code>'cap'</code> <p>Attributes:</p> Name Type Description <code>limit</code> <code>int</code> <p>The maximum requests allowed within the sliding window.</p> <code>window_seconds</code> <code>int</code> <p>The total calculated window size in seconds.</p> <code>lua_script</code> <code>str</code> <p>The Lua script used for the log-based sliding window logic in Redis.</p> <code>_instance_id</code> <code>str</code> <p>A unique identifier for this limiter instance, used to create distinct Redis keys for isolation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>limit</code> is not positive or if the calculated <code>window_seconds</code> is not positive (i.e., all time units are zero).</p> Note <p>This implementation uses Redis sorted sets (<code>ZADD</code>, <code>ZREMRANGEBYSCORE</code>, <code>ZCARD</code>) to store and manage request timestamps, ensuring atomic operations for accurate rate limiting.</p> Source code in <code>fastapicap/strategy/sliding_window_log.py</code> <pre><code>class SlidingWindowLogRateLimiter(BaseLimiter):\n    \"\"\"\n    Implements a **Sliding Window (Log-based)** rate limiting algorithm.\n\n    This is the most accurate form of the sliding window algorithm. It works by\n    storing a timestamp for every request made by a client within a Redis sorted set.\n    When a new request comes in, the algorithm first removes all timestamps that\n    fall outside the current sliding window. Then, it counts the number of remaining\n    timestamps within the window. If this count is below the `limit`, the request\n    is allowed, and its timestamp is added to the set. This method ensures\n    precise rate limiting as the window truly \"slides\" over time.\n\n    Args:\n        limit (int): The maximum number of requests allowed within the defined\n            sliding window. Must be a positive integer.\n        window_seconds (int): The number of seconds defining the size of the\n            sliding window. Can be combined with minutes, hours, or days.\n            Defaults to 0.\n        window_minutes (int): The number of minutes defining the window size.\n            Defaults to 0.\n        window_hours (int): The number of hours defining the window size.\n            Defaults to 0.\n        window_days (int): The number of days defining the window size.\n            Defaults to 0.\n        key_func (Optional[Callable[[Request], str]]): An asynchronous or\n            synchronous function to extract a unique key from the request.\n            Defaults to client IP and path. The function should accept\n            a `fastapi.Request` object and return a `str`.\n        on_limit (Optional[Callable[[Request, Response, int], None]]): An\n            asynchronous or synchronous function called when the rate limit is exceeded.\n            Defaults to raising HTTP 429. The function should accept a\n            `fastapi.Request`, `fastapi.Response`, and an `int` (retry_after seconds),\n            and should not return a value.\n        prefix (str): Redis key prefix for all limiter keys.\n            Defaults to \"cap\".\n\n    Attributes:\n        limit (int): The maximum requests allowed within the sliding window.\n        window_seconds (int): The total calculated window size in seconds.\n        lua_script (str): The Lua script used for the log-based sliding window\n            logic in Redis.\n        _instance_id (str): A unique identifier for this limiter instance, used\n            to create distinct Redis keys for isolation.\n\n    Raises:\n        ValueError: If the `limit` is not positive or if the calculated\n            `window_seconds` is not positive (i.e., all time units are zero).\n\n    Note:\n        This implementation uses Redis sorted sets (`ZADD`, `ZREMRANGEBYSCORE`, `ZCARD`)\n        to store and manage request timestamps, ensuring atomic operations\n        for accurate rate limiting.\n    \"\"\"\n    def __init__(\n        self,\n        limit: int,\n        window_seconds: int = 0,\n        window_minutes: int = 0,\n        window_hours: int = 0,\n        window_days: int = 0,\n        key_func: Optional[Callable[[Request], str]] = None,\n        on_limit: Optional[Callable[[Request, Response, int], None]] = None,\n        prefix: str = \"cap\",\n    ):\n        super().__init__(key_func=key_func, on_limit=on_limit, prefix=prefix)\n        self.limit = limit\n        if limit &lt;= 0:\n            raise ValueError(\"Limit must be a positive integer.\")\n        self.window_seconds = (\n            window_seconds\n            + window_minutes * 60\n            + window_hours * 3600\n            + window_days * 86400\n        )\n        if self.window_seconds &lt;= 0:\n            raise ValueError(\n                \"Window must be positive (set seconds, minutes, hours, or days)\"\n            )\n        self.lua_script = SLIDING_LOG_LUA\n        self._instance_id = f\"sliding_log_{id(self)}\"\n\n    async def __call__(self, request: Request, response: Response):\n        \"\"\"\n        Applies the log-based sliding window rate limiting logic to the incoming request.\n\n        This method is the core of the rate limiter. It interacts with Redis to\n        manage request timestamps within a sorted set and checks if the total\n        count within the current sliding window exceeds the configured limit.\n\n        Args:\n            request (Request): The incoming FastAPI request object.\n            response (Response): The FastAPI response object. This can be\n                modified by the `on_limit` handler if needed.\n\n        Raises:\n            HTTPException: By default, if the rate limit is exceeded,\n                `BaseLimiter._default_on_limit` will raise an `HTTPException`\n                with status code 429. Custom `on_limit` functions may raise\n                other exceptions or handle the response differently.\n        \"\"\"\n        redis = self._ensure_redis()\n        await self._ensure_lua_sha(self.lua_script)\n        key: str = await self._safe_call(self.key_func, request)\n        full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n        now = int(time.time() * 1000)\n        window_ms = self.window_seconds * 1000\n        result = await redis.evalsha(\n            self.lua_sha,\n            1,\n            full_key,\n            str(now),\n            str(window_ms),\n            str(self.limit),\n        )\n        allowed = result == 1\n        retry_after = 0 if allowed else int(result)\n        if not allowed:\n            await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"api/#fastapicap.SlidingWindowLogRateLimiter.__call__","title":"<code>__call__(request, response)</code>  <code>async</code>","text":"<p>Applies the log-based sliding window rate limiting logic to the incoming request.</p> <p>This method is the core of the rate limiter. It interacts with Redis to manage request timestamps within a sorted set and checks if the total count within the current sliding window exceeds the configured limit.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The incoming FastAPI request object.</p> required <code>response</code> <code>Response</code> <p>The FastAPI response object. This can be modified by the <code>on_limit</code> handler if needed.</p> required <p>Raises:</p> Type Description <code>HTTPException</code> <p>By default, if the rate limit is exceeded, <code>BaseLimiter._default_on_limit</code> will raise an <code>HTTPException</code> with status code 429. Custom <code>on_limit</code> functions may raise other exceptions or handle the response differently.</p> Source code in <code>fastapicap/strategy/sliding_window_log.py</code> <pre><code>async def __call__(self, request: Request, response: Response):\n    \"\"\"\n    Applies the log-based sliding window rate limiting logic to the incoming request.\n\n    This method is the core of the rate limiter. It interacts with Redis to\n    manage request timestamps within a sorted set and checks if the total\n    count within the current sliding window exceeds the configured limit.\n\n    Args:\n        request (Request): The incoming FastAPI request object.\n        response (Response): The FastAPI response object. This can be\n            modified by the `on_limit` handler if needed.\n\n    Raises:\n        HTTPException: By default, if the rate limit is exceeded,\n            `BaseLimiter._default_on_limit` will raise an `HTTPException`\n            with status code 429. Custom `on_limit` functions may raise\n            other exceptions or handle the response differently.\n    \"\"\"\n    redis = self._ensure_redis()\n    await self._ensure_lua_sha(self.lua_script)\n    key: str = await self._safe_call(self.key_func, request)\n    full_key = f\"{self.prefix}:{self._instance_id}:{key}\"\n    now = int(time.time() * 1000)\n    window_ms = self.window_seconds * 1000\n    result = await redis.evalsha(\n        self.lua_sha,\n        1,\n        full_key,\n        str(now),\n        str(window_ms),\n        str(self.limit),\n    )\n    allowed = result == 1\n    retry_after = 0 if allowed else int(result)\n    if not allowed:\n        await self._safe_call(self.on_limit, request, response, retry_after)\n</code></pre>"},{"location":"quickstart/","title":"FastAPI Cap Quick Start","text":"<p>FastAPI Cap is a robust, extensible rate limiting library for FastAPI, powered by Redis. This guide will help you get started quickly with the Fixed Window rate limiting strategy.</p>"},{"location":"quickstart/#1-installation","title":"1. Installation","text":"<p>Install the package using pip:</p> <pre><code>pip install fastapicap\n</code></pre> <p>Note: You also need a running Redis instance. You can run one locally using Docker:</p>"},{"location":"quickstart/#2-initialize-the-app","title":"2. Initialize the App","text":"<p>Before using any limiter, you must initialize the shared Redis connection. You can do this in your FastAPI app's lifespan event or directly at startup.</p> <pre><code>from fastapi import FastAPI\nfrom fastapicap import Cap\n\napp = FastAPI()\nCap.init_app(\"redis://localhost:6379/0\")\n\n</code></pre>"},{"location":"quickstart/#3-using-the-fixed-window-rate-limiter","title":"3. Using the Fixed Window Rate Limiter","text":"<p>Import the <code>RateLimiter</code> (Also Known As Fixed Window Rate Limiter) and use it as a dependency on your route:</p> <pre><code>from fastapi import Depends\nlimiter1 = RateLimiter(limit=5, minutes=1)\n\n@app.get(\"/limited\", dependencies=[Depends(limiter1)])\nasync def limited_endpoint():\n    return {\"message\": \"You are within the rate limit!\"}\n</code></pre> <p>If the limit is exceeded, the client receives a <code>429 Too Many Requests</code> response with a <code>Retry-After</code> header.</p>"},{"location":"quickstart/#4-using-multiple-limiters-on-a-single-route","title":"4. Using Multiple Limiters on a Single Route","text":"<p>You can combine multiple limiters for more granular control. For example, to allow 1 request per second and 30 requests per minute:</p> <pre><code>from fastapi import FastAPI, Depends\nfrom fastapicap import Cap, RateLimiter\n\n## App Init part\n\nlimiter_1s = RateLimiter(limit=1, seconds=1)\nlimiter_30m = RateLimiter(limit=30, minutes=1)\n\n@app.get(\"/strict\", dependencies=[Depends(limiter_1s), Depends(limiter_30m)])\nasync def strict_endpoint():\n    return {\"message\": \"You passed both rate limits!\"}\n</code></pre> <p>If either limit is exceeded, the request will be blocked.</p>"},{"location":"quickstart/#notes","title":"Notes","text":"<ul> <li>The default key for rate limiting is based on the client IP and request path.</li> <li>All limiters are backed by Redis and require a working Redis connection.</li> <li>Only dependency-based usage is currently supported and tested.</li> </ul>"},{"location":"quickstart/#5-customizing-on_limit-and-key_func-in-fastapi-cap","title":"5. Customizing <code>on_limit</code> and <code>key_func</code> in FastAPI Cap","text":"<p>FastAPI Cap allows you to customize how rate limits are enforced and how unique clients are identified by providing your own <code>on_limit</code> and <code>key_func</code> functions to any limiter. This guide explains how to use and implement custom <code>on_limit</code> and <code>key_func</code> functions, including the parameters they receive.</p>"},{"location":"quickstart/#default-key_func-implementation","title":"Default <code>key_func</code> Implementation","text":"<p>By default, FastAPI Cap uses the client IP address and request path to generate a unique key for each client and endpoint.</p> <pre><code>@staticmethod\nasync def _default_key_func(request: Request) -&gt; str:\n    \"\"\"\n    Default key function: uses client IP and request path.\n    \"\"\"\n    x_forwarded_for = request.headers.get(\"X-Forwarded-For\")\n    if x_forwarded_for:\n        client_ip = x_forwarded_for.split(\",\")[0].strip()\n    else:\n        client_ip = request.client.host if request.client else \"unknown\"\n    return f\"{client_ip}:{request.url.path}\"\n</code></pre> <ul> <li>Parameters: </li> <li><code>request</code>: The FastAPI <code>Request</code> object.</li> <li>Returns: </li> <li>A string key in the format <code>client_ip:/path</code>.</li> </ul>"},{"location":"quickstart/#default-on_limit-implementation","title":"Default <code>on_limit</code> Implementation","text":"<p>By default, FastAPI Cap raises a <code>429 Too Many Requests</code> HTTPException and sets the <code>Retry-After</code> header.</p> <pre><code>@staticmethod\nasync def _default_on_limit(request, response, retry_after: int) -&gt; None:\n    \"\"\"\n    Default handler when the rate limit is exceeded.\n    \"\"\"\n    from fastapi import HTTPException\n\n    raise HTTPException(\n        status_code=429,\n        detail=\"Rate limit exceeded. Please try again later.\",\n        headers={\"Retry-After\": str(retry_after)},\n    )\n</code></pre> <ul> <li>Parameters: </li> <li><code>request</code>: The FastAPI <code>Request</code> object.</li> <li><code>response</code>: The FastAPI <code>Response</code> object (not used in the default).</li> <li><code>retry_after</code>: An integer indicating how many seconds to wait before retrying.</li> </ul>"},{"location":"quickstart/#custom-key_func","title":"Custom <code>key_func</code>","text":"<p>The <code>key_func</code> is responsible for generating a unique key for each client/request. You can provide your own logic to rate limit by user ID, API key, or any other identifier.</p>"},{"location":"quickstart/#signature","title":"Signature","text":"<pre><code>async def key_func(request: Request) -&gt; str:\n    ...\n</code></pre>"},{"location":"quickstart/#example-rate-limit-by-user-id","title":"Example: Rate Limit by User ID","text":"<pre><code>from fastapi import Request\n\nasync def user_id_key_func(request: Request) -&gt; str:\n    # Assume user ID is stored in request.state.user_id\n    user_id = getattr(request.state, \"user_id\", None)\n    if user_id is None:\n        # Fallback to IP if user is not authenticated\n        client_ip = request.client.host if request.client else \"unknown\"\n        return f\"anon:{client_ip}:{request.url.path}\"\n    return f\"user:{user_id}:{request.url.path}\"\n</code></pre> <p>Usage:</p> <pre><code>limiter = RateLimiter(limit=10, minutes=1, key_func=user_id_key_func)\n</code></pre>"},{"location":"quickstart/#using-your-custom-on_limit-handler","title":"Using Your Custom <code>on_limit</code>  Handler","text":"<p>The <code>on_limit</code> function is called when a client exceeds the rate limit. You can customize this to log, return a custom response, or perform other actions.</p>"},{"location":"quickstart/#signature_1","title":"Signature","text":"<pre><code>async def on_limit(request: Request, response: Response, retry_after: int) -&gt; None:\n    ...\n</code></pre>"},{"location":"quickstart/#example-custom-json-error-response","title":"Example: Custom JSON Error Response","text":"<pre><code>from fastapi import Request, Response\nfrom fastapi.responses import JSONResponse\n\nasync def custom_on_limit(request: Request, response: Response, retry_after: int):\n    response = JSONResponse(\n        status_code=429,\n        content={\n            \"error\": \"Too many requests\",\n            \"retry_after\": retry_after,\n            \"detail\": \"Please slow down!\"\n        },\n        headers={\"Retry-After\": str(retry_after)},\n    )\n    raise response\n</code></pre> <p>Usage:</p> <pre><code>limiter = RateLimiter(limit=5, minutes=1, on_limit=custom_on_limit)\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore other strategies: Sliding Window, Token Bucket, Leaky Bucket, GCRA, and Sliding Window Log.</li> </ul> <p>Happy rate limiting!</p>"},{"location":"strategies/fixed_window/","title":"\ud83e\ude9f Fixed Window Rate Limiting","text":""},{"location":"strategies/fixed_window/#1-what-is-fixed-window-rate-limiting","title":"1. What is Fixed Window Rate Limiting?","text":"<ul> <li> <p>Concept:   Time is split into fixed intervals (e.g., 1 minute). Each client can make up to a set number of requests per interval. When the interval resets, so does the counter.</p> </li> <li> <p>Example:   If the limit is 5 requests per minute, a client can make 5 requests between 12:00:00 and 12:00:59. At 12:01:00, the counter resets.</p> </li> </ul>"},{"location":"strategies/fixed_window/#2-usage","title":"2. Usage","text":""},{"location":"strategies/fixed_window/#single-limiter-example","title":"Single Limiter Example","text":"<pre><code>from fastapicap import RateLimiter\nfrom fastapi import Depends\n\nlimiter = RateLimiter(limit=5, minutes=1)\n\n@app.get(\"/limited\", dependencies=[Depends(limiter)])\nasync def limited_endpoint():\n    return {\"message\": \"You are within the rate limit!\"}\n</code></pre>"},{"location":"strategies/fixed_window/#multiple-limiters-example","title":"Multiple Limiters Example","text":"<pre><code>limiter_1s = RateLimiter(limit=1, seconds=1)\nlimiter_30m = RateLimiter(limit=30, minutes=1)\n\n@app.get(\"/strict\", dependencies=[Depends(limiter_1s), Depends(limiter_30m)])\nasync def strict_endpoint():\n    return {\"message\": \"You passed both rate limits!\"}\n</code></pre>"},{"location":"strategies/fixed_window/#3-available-configuration-options","title":"3. Available Configuration Options","text":"<p>You can customize the Fixed Window limiter using the following parameters:</p> Parameter Type Description Default <code>limit</code> <code>int</code> Required. Maximum number of requests allowed per window. Must be positive. \u2014 <code>seconds</code> <code>int</code> Number of seconds in the window. Can be combined with minutes, hours, or days. <code>0</code> <code>minutes</code> <code>int</code> Number of minutes in the window. <code>0</code> <code>hours</code> <code>int</code> Number of hours in the window. <code>0</code> <code>days</code> <code>int</code> Number of days in the window. <code>0</code> <code>key_func</code> <code>Callable</code> Function to extract a unique key from the request (e.g., by IP, user ID, etc.). By default, uses client IP and path. <code>on_limit</code> <code>Callable</code> Function called when the rate limit is exceeded. By default, raises HTTP 429. <code>prefix</code> <code>str</code> Redis key prefix for all limiter keys. <code>\"cap\"</code> <p>Note: - The window size is calculated as the sum of all time units provided (<code>seconds</code>, <code>minutes</code>, <code>hours</code>, <code>days</code>). - At least one time unit must be positive, and <code>limit</code> must be positive.</p> <p>Example:</p> <pre><code># 100 requests per hour, with a custom Redis key prefix\nlimiter = RateLimiter(limit=100, hours=1, prefix=\"myapi\")\n</code></pre>"},{"location":"strategies/fixed_window/#4-how-fixed-window-works","title":"4. How Fixed Window Works","text":"<p>Suppose you set a limit of 5 requests per minute:</p> <ul> <li>At 12:00:00, the window starts.</li> <li>The client makes 5 requests between 12:00:00 and 12:00:59 \u2014 all are allowed.</li> <li>A 6th request at 12:00:30 is blocked with a 429 error.</li> <li>At 12:01:00, the window resets. The client can make 5 more requests.</li> </ul> <p>Visualization:</p> Time Request # Allowed? Reason 12:00:01 1 \u2705 Within limit 12:00:10 2 \u2705 Within limit 12:00:20 3 \u2705 Within limit 12:00:30 4 \u2705 Within limit 12:00:50 5 \u2705 Within limit 12:00:55 6 \u274c Limit exceeded 12:01:01 7 \u2705 New window, allowed"},{"location":"strategies/fixed_window/#5-notes-pros-cons","title":"5. Notes, Pros &amp; Cons","text":"<p>Pros: - Simple to understand and implement. - Low memory usage. - Good for aggressive, clear-cut limits.</p> <p>Cons: - Can allow bursts at window boundaries (e.g., 5 requests at 12:00:59 and 5 more at 12:01:00).</p> <p>Use Fixed Window for simple, aggressive limits where bursts at window boundaries are acceptable.</p>"},{"location":"strategies/gcra/","title":"\u26a1 GCRA (Generic Cell Rate Algorithm) Rate Limiting","text":""},{"location":"strategies/gcra/#1-what-is-gcra-rate-limiting","title":"1. What is GCRA Rate Limiting?","text":"<ul> <li> <p>Concept:   The Generic Cell Rate Algorithm (GCRA) is a precise, burstable rate limiting algorithm. It tracks a \"Theoretical Arrival Time\" (TAT) for each client and allows requests as long as they do not arrive \"too early\" according to the configured rate. GCRA is similar to the Token Bucket algorithm but is often more precise for burst control and fairness.</p> </li> <li> <p>Real-world usage:   GCRA is widely used in telecommunications (ATM networks), API gateways, and distributed systems where both fairness and precise burst control are required.</p> </li> </ul>"},{"location":"strategies/gcra/#2-usage","title":"2. Usage","text":""},{"location":"strategies/gcra/#single-limiter-example","title":"Single Limiter Example","text":"<pre><code>from fastapicap import GCRARateLimiter\nfrom fastapi import Depends\n\n# Allow a burst of 5 requests, then 2 requests per second\nlimiter = GCRARateLimiter(burst=5, tokens_per_second=2)\n\n@app.get(\"/gcra\", dependencies=[Depends(limiter)])\nasync def gcra_limited():\n    return {\"message\": \"You are within the GCRA rate limit!\"}\n</code></pre>"},{"location":"strategies/gcra/#multiple-limiters-example","title":"Multiple Limiters Example","text":"<pre><code>limiter_short = GCRARateLimiter(burst=2, tokens_per_second=1)\nlimiter_long = GCRARateLimiter(burst=10, tokens_per_minute=30)\n\n@app.get(\"/multi-gcra\", dependencies=[Depends(limiter_short), Depends(limiter_long)])\nasync def multi_gcra_limited():\n    return {\"message\": \"You passed both GCRA rate limits!\"}\n</code></pre>"},{"location":"strategies/gcra/#3-available-configuration-options","title":"3. Available Configuration Options","text":"<p>You can customize the GCRA limiter using the following parameters:</p> Parameter Type Description Default <code>burst</code> <code>int</code> Required. Maximum number of requests that can be served instantly (burst capacity). \u2014 <code>tokens_per_second</code> <code>float</code> Number of tokens allowed per second (steady rate). <code>0</code> <code>tokens_per_minute</code> <code>float</code> Number of tokens allowed per minute. <code>0</code> <code>tokens_per_hour</code> <code>float</code> Number of tokens allowed per hour. <code>0</code> <code>tokens_per_day</code> <code>float</code> Number of tokens allowed per day. <code>0</code> <code>key_func</code> <code>Callable</code> Function to extract a unique key from the request. By default, uses client IP and path. <code>on_limit</code> <code>Callable</code> Function called when the rate limit is exceeded. By default, raises HTTP 429. <code>prefix</code> <code>str</code> Redis key prefix for all limiter keys. <code>\"cap\"</code> <p>Note: - The total steady rate is the sum of all <code>tokens_per_*</code> arguments, converted to tokens per second. - At least one steady rate must be positive, and <code>burst</code> must be positive.</p> <p>Example:</p> <pre><code># Burst of 20, steady rate of 100 per hour, with a custom prefix\nlimiter = GCRARateLimiter(burst=20, tokens_per_hour=100, prefix=\"myapi\")\n</code></pre>"},{"location":"strategies/gcra/#4-how-gcra-works-with-example","title":"4. How GCRA Works (with Example)","text":"<p>Suppose you set a burst of 5 and a steady rate of 2 requests per second.</p> <ul> <li>If the client has been idle, they can make up to 5 requests instantly (burst).</li> <li>After the burst, requests are only allowed at the steady rate (2 per second).</li> <li>If requests arrive too quickly, the limiter calculates how long the client must wait before the next request is allowed (<code>retry_after</code>).</li> </ul> <p>Visualization:</p> Time Burst Left Allowed? Reason Retry-After (s) 12:00:00.000 5 \u2705 Burst 0 12:00:00.100 4 \u2705 Burst 0 12:00:00.200 3 \u2705 Burst 0 12:00:00.300 2 \u2705 Burst 0 12:00:00.400 1 \u2705 Burst 0 12:00:00.500 0 \u274c Rate exceeded 0.5 12:00:01.000 1 \u2705 Steady rate resumes 0 <ul> <li>After the burst is used, requests are only allowed at the configured steady rate.</li> </ul>"},{"location":"strategies/gcra/#5-notes-pros-cons","title":"5. Notes, Pros &amp; Cons","text":"<p>Notes:</p> <ul> <li>GCRA is highly accurate and fair, making it ideal for per-user or per-client rate limiting.</li> <li>The <code>retry_after</code> value is precise, based on the theoretical arrival time (TAT).</li> </ul> <p>Pros:</p> <ul> <li>Precise burst and rate control.</li> <li>Smooth, fair distribution of requests over time.</li> <li>Prevents starvation and ensures fairness.</li> </ul> <p>Cons:</p> <ul> <li>More complex to understand and configure than Fixed Window or Token Bucket.</li> <li>Requires careful tuning of burst and steady rate for best results.</li> </ul> <p>Use GCRA when you need precise, fair, and burstable rate limiting for your APIs or distributed systems.</p>"},{"location":"strategies/leaky_bucket/","title":"\ud83e\udea3 Leaky Bucket Rate Limiting","text":""},{"location":"strategies/leaky_bucket/#1-what-is-leaky-bucket-rate-limiting","title":"1. What is Leaky Bucket Rate Limiting?","text":"<ul> <li> <p>Concept:   The Leaky Bucket algorithm models traffic flow like water in a bucket. Each request is a \"drop\" added to the bucket. The bucket leaks at a constant rate (the leak rate), and if the bucket overflows (i.e., too many requests arrive too quickly), new requests are rejected. This produces a smooth, constant output rate and prevents bursts from overwhelming downstream systems.</p> </li> <li> <p>Real-world usage:   Leaky Bucket is commonly used in network routers, firewalls, and API gateways to ensure a steady flow of requests and to protect backend services from sudden spikes in traffic.</p> </li> </ul>"},{"location":"strategies/leaky_bucket/#2-usage","title":"2. Usage","text":""},{"location":"strategies/leaky_bucket/#single-limiter-example","title":"Single Limiter Example","text":"<pre><code>from fastapicap import LeakyBucketRateLimiter\nfrom fastapi import Depends\n\n# Allow a burst of up to 10 requests, leaking at 2 requests per second\nlimiter = LeakyBucketRateLimiter(capacity=10, leaks_per_second=2)\n\n@app.get(\"/leaky-bucket\", dependencies=[Depends(limiter)])\nasync def leaky_bucket_limited():\n    return {\"message\": \"You are within the leaky bucket rate limit!\"}\n</code></pre>"},{"location":"strategies/leaky_bucket/#multiple-limiters-example","title":"Multiple Limiters Example","text":"<pre><code>limiter_short = LeakyBucketRateLimiter(capacity=5, leaks_per_second=1)\nlimiter_long = LeakyBucketRateLimiter(capacity=30, leaks_per_minute=10)\n\n@app.get(\"/multi-leaky-bucket\", dependencies=[Depends(limiter_short), Depends(limiter_long)])\nasync def multi_leaky_bucket_limited():\n    return {\"message\": \"You passed both leaky bucket rate limits!\"}\n</code></pre>"},{"location":"strategies/leaky_bucket/#3-available-configuration-options","title":"3. Available Configuration Options","text":"<p>You can customize the Leaky Bucket limiter using the following parameters:</p> Parameter Type Description Default <code>capacity</code> <code>int</code> Required. Maximum number of requests the bucket can hold (burst size). Must be positive. \u2014 <code>leaks_per_second</code> <code>float</code> Number of requests leaked (processed) per second. <code>0</code> <code>leaks_per_minute</code> <code>float</code> Number of requests leaked per minute. <code>0</code> <code>leaks_per_hour</code> <code>float</code> Number of requests leaked per hour. <code>0</code> <code>leaks_per_day</code> <code>float</code> Number of requests leaked per day. <code>0</code> <code>key_func</code> <code>Callable</code> Function to extract a unique key from the request. By default, uses client IP and path. <code>on_limit</code> <code>Callable</code> Function called when the rate limit is exceeded. By default, raises HTTP 429. <code>prefix</code> <code>str</code> Redis key prefix for all limiter keys. <code>\"cap\"</code> <p>Note: - The total leak rate is the sum of all <code>leaks_per_*</code> arguments, converted to requests per second. - At least one leak rate must be positive, and <code>capacity</code> must be positive.</p> <p>Example:</p> <pre><code># 100 request burst, leaking at 10 requests per minute, with a custom prefix\nlimiter = LeakyBucketRateLimiter(capacity=100, leaks_per_minute=10, prefix=\"myapi\")\n</code></pre>"},{"location":"strategies/leaky_bucket/#4-how-leaky-bucket-works-with-example","title":"4. How Leaky Bucket Works (with Example)","text":"<p>Suppose you set a capacity of 10 and a leak rate of 2 requests per second.</p> <ul> <li>The bucket starts empty.</li> <li>Each request adds a \"drop\" to the bucket.</li> <li>If 10 requests arrive instantly, all are accepted (bucket is now full).</li> <li>If more requests arrive before the bucket has leaked enough, they are rejected.</li> <li>The bucket leaks at a constant rate (2 requests per second), making space for new requests.</li> </ul> <p>Visualization:</p> Time Bucket Level Request? Allowed? Reason 12:00:00.000 0 Yes \u2705 Bucket not full 12:00:00.100 1 Yes \u2705 ... ... ... ... ... 12:00:00.900 9 Yes \u2705 12:00:01.000 10 Yes \u2705 Bucket full 12:00:01.100 10 Yes \u274c Bucket still full 12:00:01.500 9 Yes \u2705 1 request leaked out <ul> <li>The bucket \"leaks\" at a constant rate, so after 0.5 seconds, 1 request has leaked out, making space for a new request.</li> </ul>"},{"location":"strategies/leaky_bucket/#5-notes-pros-cons","title":"5. Notes, Pros &amp; Cons","text":"<p>Notes:</p> <ul> <li>This strategy is ideal for smoothing out bursty traffic and ensuring a constant request rate to downstream services.</li> <li>The <code>retry_after</code> value is based on how long until the next \"drop\" leaks out.</li> </ul> <p>Pros:</p> <ul> <li>Produces a steady, predictable output rate.</li> <li>Prevents bursts from overwhelming your backend.</li> <li>Simple and effective for traffic shaping.</li> </ul> <p>Cons:</p> <ul> <li>Does not allow true bursts beyond the bucket capacity.</li> <li>May introduce latency if requests are queued up in the bucket.</li> <li>Configuration (capacity vs. leak rate) can be unintuitive for some use cases.</li> </ul> <p>Use Leaky Bucket when you need to smooth out bursts and enforce a constant request rate to protect downstream systems.</p>"},{"location":"strategies/overview/","title":"Overview","text":"<p># \ud83e\udde9 Rate Limiting Strategies in FastAPI-Cap</p> <p>FastAPI-Cap provides a suite of powerful, production-grade rate limiting strategies, each designed to address different API traffic patterns and business requirements. All strategies are implemented using Redis for distributed state management and Lua scripting for atomic, high-performance operations.</p> <p>This page introduces each available strategy, helping you choose the best fit for your use case.</p>"},{"location":"strategies/overview/#available-strategies","title":"\ud83d\udcda Available Strategies","text":""},{"location":"strategies/overview/#1-fixed-window","title":"1. Fixed Window","text":"<p>Description: Divides time into fixed intervals (windows), counting requests within each window. The counter resets at the start of each new window.</p> <ul> <li>Best for: Simple, aggressive limits (e.g., \"100 requests per minute\").</li> <li>Pros: Easy to understand and implement; low memory usage.</li> <li>Cons: Can allow bursts at window boundaries.</li> </ul> <p>Learn more \u2192</p>"},{"location":"strategies/overview/#2-token-bucket","title":"2. Token Bucket","text":"<p>Description: Tokens are added to a bucket at a steady rate. Each request consumes a token. Allows short bursts up to the bucket's capacity, then enforces an average rate.</p> <ul> <li>Best for: Allowing bursts while maintaining a steady average rate.</li> <li>Pros: Smooths traffic; flexible burst control.</li> <li>Cons: Requires tuning of capacity and refill rate.</li> </ul> <p>Learn more \u2192</p>"},{"location":"strategies/overview/#3-leaky-bucket","title":"3. Leaky Bucket","text":"<p>Description: Requests are added to a bucket and \"leak\" out at a constant rate. If the bucket overflows, new requests are rejected.</p> <ul> <li>Best for: Enforcing a constant output rate; smoothing out bursts.</li> <li>Pros: Produces a steady request flow.</li> <li>Cons: Does not allow bursts; may introduce latency.</li> </ul> <p>Learn more \u2192</p>"},{"location":"strategies/overview/#4-generic-cell-rate-algorithm-gcra","title":"4. Generic Cell Rate Algorithm (GCRA)","text":"<p>Description: A precise algorithm that tracks a \"Theoretical Arrival Time\" for each request, providing fine-grained burst and rate control.</p> <ul> <li>Best for: Precise, fair rate limiting with accurate burst handling.</li> <li>Pros: Highly accurate; prevents starvation.</li> <li>Cons: More complex to configure and understand.</li> </ul> <p>Learn more \u2192</p>"},{"location":"strategies/overview/#5-approximated-sliding-window","title":"5. Approximated Sliding Window","text":"<p>Description: Estimates the number of requests in a sliding window using two fixed windows and a weighted average.</p> <ul> <li>Best for: Balancing accuracy and resource usage.</li> <li>Pros: Smoother than fixed window; more efficient than log-based.</li> <li>Cons: Not perfectly accurate; some boundary effects.</li> </ul> <p>Learn more \u2192</p>"},{"location":"strategies/overview/#6-sliding-window-log-based","title":"6. Sliding Window (Log-based)","text":"<p>Description: Records a timestamp for every request and counts only those within the current window, providing the most accurate and fair rate limiting.</p> <ul> <li>Best for: Maximum accuracy and fairness.</li> <li>Pros: Eliminates burst issues; ensures consistent rate.</li> <li>Cons: Higher memory usage for high-traffic endpoints.</li> </ul> <p>Learn more \u2192</p>"},{"location":"strategies/overview/#how-to-choose","title":"\ud83d\udee0\ufe0f How to Choose?","text":"<ul> <li>For simple, low-traffic APIs: Start with Fixed Window.</li> <li>For APIs needing burst tolerance: Use Token Bucket or GCRA.</li> <li>For strict, smooth traffic shaping: Try Leaky Bucket.</li> <li>For maximum fairness and accuracy: Use Sliding Window (Log-based).</li> <li>For a balance of accuracy and efficiency: Consider Approximated Sliding Window.</li> </ul>"},{"location":"strategies/overview/#next-steps","title":"\ud83d\udcd6 Next Steps","text":"<ul> <li>Jump to the Quickstart Guide</li> <li>API Reference</li> </ul>"},{"location":"strategies/sliding_window/","title":"\ud83e\ude9f Approximated Sliding Window Rate Limiting","text":""},{"location":"strategies/sliding_window/#1-what-is-approximated-sliding-window-rate-limiting","title":"1. What is Approximated Sliding Window Rate Limiting?","text":"<ul> <li> <p>Concept:   The Approximated Sliding Window algorithm provides smoother and more accurate rate limiting than a simple Fixed Window, while being more memory-efficient than a log-based sliding window.   It works by maintaining counters for the current and previous fixed windows. The effective count for the sliding window is calculated as a weighted sum of requests in both windows, based on how much of the previous window still \"slides\" into the current view.</p> </li> <li> <p>Example:   If you set a limit of 10 requests per minute, the limiter will estimate the number of requests in the last 60 seconds by combining the counts from the current and previous minute, weighted by how much of each window overlaps with the current time.</p> </li> </ul>"},{"location":"strategies/sliding_window/#2-usage","title":"2. Usage","text":""},{"location":"strategies/sliding_window/#single-limiter-example","title":"Single Limiter Example","text":"<pre><code>from fastapicap import SlidingWindowRateLimiter\nfrom fastapi import Depends\n\nlimiter = SlidingWindowRateLimiter(limit=10, minutes=1)\n\n@app.get(\"/sliding\", dependencies=[Depends(limiter)])\nasync def sliding_limited():\n    return {\"message\": \"You are within the sliding window rate limit!\"}\n</code></pre>"},{"location":"strategies/sliding_window/#multiple-limiters-example","title":"Multiple Limiters Example","text":"<pre><code>limiter_5s = SlidingWindowRateLimiter(limit=2, seconds=5)\nlimiter_1m = SlidingWindowRateLimiter(limit=10, minutes=1)\n\n@app.get(\"/multi-sliding\", dependencies=[Depends(limiter_5s), Depends(limiter_1m)])\nasync def multi_sliding_limited():\n    return {\"message\": \"You passed both sliding window rate limits!\"}\n</code></pre>"},{"location":"strategies/sliding_window/#3-available-configuration-options","title":"3. Available Configuration Options","text":"<p>You can customize the Approximated Sliding Window limiter using the following parameters:</p> Parameter Type Description Default <code>limit</code> <code>int</code> Required. Maximum number of requests allowed within the sliding window. Must be positive. \u2014 <code>seconds</code> <code>int</code> Number of seconds in the window segment. Can be combined with minutes, hours, or days. <code>0</code> <code>minutes</code> <code>int</code> Number of minutes in the window segment. <code>0</code> <code>hours</code> <code>int</code> Number of hours in the window segment. <code>0</code> <code>days</code> <code>int</code> Number of days in the window segment. <code>0</code> <code>key_func</code> <code>Callable</code> Function to extract a unique key from the request. By default, uses client IP and path. <code>on_limit</code> <code>Callable</code> Function called when the rate limit is exceeded. By default, raises HTTP 429. <code>prefix</code> <code>str</code> Redis key prefix for all limiter keys. <code>\"cap\"</code> <p>Note: - The window size is calculated as the sum of all time units provided (<code>seconds</code>, <code>minutes</code>, <code>hours</code>, <code>days</code>). - At least one time unit must be positive, and <code>limit</code> must be positive.</p> <p>Example:</p> <pre><code># 100 requests per hour, with a custom Redis key prefix\nlimiter = SlidingWindowRateLimiter(limit=100, hours=1, prefix=\"myapi\")\n</code></pre>"},{"location":"strategies/sliding_window/#4-how-approximated-sliding-window-works-with-example","title":"4. How Approximated Sliding Window Works (with Example)","text":"<p>Suppose you set a limit of 10 requests per minute.</p> <ul> <li>The limiter keeps two counters: one for the current minute and one for the previous minute.</li> <li>When a request arrives, it calculates how much of the previous window overlaps with the current 60-second period and combines the counts accordingly.</li> <li>This provides a smoother, more accurate rate limit than a simple fixed window.</li> </ul> <p>Visualization:</p> Time Requests in Previous Window Requests in Current Window Effective Count (Weighted) Allowed? 12:00:30 4 5 4 * 0.5 + 5 = 7 \u2705 12:00:45 4 7 4 * 0.25 + 7 = 8 \u2705 12:00:59 4 10 4 * 0.01 + 10 \u2248 10.04 \u274c <ul> <li>The \"Effective Count\" is the sum of the current window's count plus a weighted portion of the previous window's count, based on how much of the previous window still overlaps with the sliding window.</li> </ul>"},{"location":"strategies/sliding_window/#5-notes-pros-cons","title":"5. Notes, Pros &amp; Cons","text":"<p>Notes: - This strategy is more accurate and smoother than Fixed Window, but more memory-efficient than log-based sliding window. - The <code>retry_after</code> value is an approximation.</p> <p>Pros: - Smoother rate limiting than Fixed Window. - More memory-efficient than log-based sliding window. - Good balance of accuracy and performance.</p> <p>Cons: - Not perfectly accurate (it's an approximation). - Still susceptible to some boundary effects if not carefully tuned.</p> <p>Use Approximated Sliding Window when you want smoother, fairer rate limiting than Fixed Window, but don't need the full accuracy (or memory cost) of a log-based sliding window.</p>"},{"location":"strategies/sliding_window_log/","title":"\ud83e\ude9f Sliding Window (Log-based) Rate Limiting","text":""},{"location":"strategies/sliding_window_log/#1-what-is-sliding-window-log-based-rate-limiting","title":"1. What is Sliding Window (Log-based) Rate Limiting?","text":"<ul> <li> <p>Concept:   The Sliding Window (Log-based) algorithm is the most accurate form of sliding window rate limiting. It stores a timestamp for every request in a Redis sorted set. When a new request arrives, it removes all timestamps outside the current window, counts the remaining ones, and allows or blocks the request based on the configured limit. This ensures the window truly \"slides\" over time, providing precise and fair rate limiting.</p> </li> <li> <p>Example:   If you set a limit of 10 requests per minute, the limiter will only allow 10 requests in any rolling 60-second period, regardless of when those requests occur.</p> </li> </ul>"},{"location":"strategies/sliding_window_log/#2-usage","title":"2. Usage","text":""},{"location":"strategies/sliding_window_log/#single-limiter-example","title":"Single Limiter Example","text":"<pre><code>from fastapicap import SlidingWindowLogRateLimiter\nfrom fastapi import Depends\n\nlimiter = SlidingWindowLogRateLimiter(limit=10, window_minutes=1)\n\n@app.get(\"/sliding-log\", dependencies=[Depends(limiter)])\nasync def sliding_log_limited():\n    return {\"message\": \"You are within the log-based sliding window rate limit!\"}\n</code></pre>"},{"location":"strategies/sliding_window_log/#multiple-limiters-example","title":"Multiple Limiters Example","text":"<pre><code>limiter_10s = SlidingWindowLogRateLimiter(limit=3, window_seconds=10)\nlimiter_1m = SlidingWindowLogRateLimiter(limit=10, window_minutes=1)\n\n@app.get(\"/multi-sliding-log\", dependencies=[Depends(limiter_10s), Depends(limiter_1m)])\nasync def multi_sliding_log_limited():\n    return {\"message\": \"You passed both log-based sliding window rate limits!\"}\n</code></pre>"},{"location":"strategies/sliding_window_log/#3-available-configuration-options","title":"3. Available Configuration Options","text":"<p>You can customize the Sliding Window (Log-based) limiter using the following parameters:</p> Parameter Type Description Default <code>limit</code> <code>int</code> Required. Maximum number of requests allowed within the sliding window. Must be positive. \u2014 <code>window_seconds</code> <code>int</code> Number of seconds in the sliding window. Can be combined with minutes, hours, or days. <code>0</code> <code>window_minutes</code> <code>int</code> Number of minutes in the sliding window. <code>0</code> <code>window_hours</code> <code>int</code> Number of hours in the sliding window. <code>0</code> <code>window_days</code> <code>int</code> Number of days in the sliding window. <code>0</code> <code>key_func</code> <code>Callable</code> Function to extract a unique key from the request. By default, uses client IP and path. <code>on_limit</code> <code>Callable</code> Function called when the rate limit is exceeded. By default, raises HTTP 429. <code>prefix</code> <code>str</code> Redis key prefix for all limiter keys. <code>\"cap\"</code> <p>Note: - The window size is calculated as the sum of all time units provided (<code>window_seconds</code>, <code>window_minutes</code>, <code>window_hours</code>, <code>window_days</code>). - At least one time unit must be positive, and <code>limit</code> must be positive.</p> <p>Example:</p> <pre><code># 100 requests per hour, with a custom Redis key prefix\nlimiter = SlidingWindowLogRateLimiter(limit=100, window_hours=1, prefix=\"myapi\")\n</code></pre>"},{"location":"strategies/sliding_window_log/#4-how-sliding-window-log-based-works-with-example","title":"4. How Sliding Window (Log-based) Works (with Example)","text":"<p>Suppose you set a limit of 10 requests per minute.</p> <ul> <li>Every request's timestamp is stored in a Redis sorted set.</li> <li>When a new request arrives:</li> <li>All timestamps older than 60 seconds are removed.</li> <li>The number of remaining timestamps is counted.</li> <li>If the count is below 10, the request is allowed and its timestamp is added.</li> <li>If the count is 10 or more, the request is blocked.</li> </ul> <p>Visualization:</p> Time Timestamps in Window (last 60s) Allowed? Reason 12:00:01 1 \u2705 Within limit 12:00:10 2 \u2705 Within limit 12:00:20 3 \u2705 Within limit ... ... ... ... 12:00:59 10 \u2705 At limit 12:01:00 10 (oldest at 12:00:01 removed) \u2705 Oldest expired 12:01:01 10 (oldest at 12:00:10 removed) \u2705 Oldest expired 12:01:02 10 (no expired, new request) \u274c Limit exceeded <ul> <li>The window \"slides\" with every request, always considering only the last 60 seconds.</li> </ul>"},{"location":"strategies/sliding_window_log/#5-notes-pros-cons","title":"5. Notes, Pros &amp; Cons","text":"<p>Notes:</p> <ul> <li>This strategy uses Redis sorted sets for each client/key, which can increase memory usage for high-traffic endpoints.</li> <li>The <code>retry_after</code> value is precise, based on the timestamp of the oldest request in the window.</li> </ul> <p>Pros:</p> <ul> <li>Most accurate and fair rate limiting.</li> <li>Eliminates burst issues at window boundaries.</li> <li>Ensures consistent rate over any time slice within the window.</li> </ul> <p>Cons:</p> <ul> <li>Higher memory usage for high request volumes (stores a timestamp for every request in the window).</li> <li>Slightly more Redis operations per request compared to other strategies.</li> </ul> <p>Use Sliding Window (Log-based) when you need the highest accuracy and fairness in rate limiting, and can afford the extra memory usage.</p>"},{"location":"strategies/token_bucket/","title":"\ud83e\udea3 Token Bucket Rate Limiting","text":""},{"location":"strategies/token_bucket/#1-what-is-token-bucket-rate-limiting","title":"1. What is Token Bucket Rate Limiting?","text":"<ul> <li> <p>Concept:   The Token Bucket algorithm allows requests to be processed as long as there are tokens in the bucket. Tokens are added at a steady rate (the refill rate), up to a maximum capacity. Each request consumes a token. If the bucket is empty, requests are denied (or delayed). This allows for short bursts of traffic while enforcing a steady average rate over time.</p> </li> <li> <p>Example:   If you set a capacity of 10 tokens and a refill rate of 1 token per second, a client can make up to 10 requests instantly (if the bucket is full), and then 1 request per second thereafter as tokens are refilled.</p> </li> </ul>"},{"location":"strategies/token_bucket/#2-usage","title":"2. Usage","text":""},{"location":"strategies/token_bucket/#single-limiter-example","title":"Single Limiter Example","text":"<pre><code>from fastapicap import TokenBucketRateLimiter\nfrom fastapi import Depends\n\n# Allow bursts up to 10 requests, refilling at 1 token per second\nlimiter = TokenBucketRateLimiter(capacity=10, tokens_per_second=1)\n\n@app.get(\"/token-bucket\", dependencies=[Depends(limiter)])\nasync def token_bucket_limited():\n    return {\"message\": \"You are within the token bucket rate limit!\"}\n</code></pre>"},{"location":"strategies/token_bucket/#multiple-limiters-example","title":"Multiple Limiters Example","text":"<pre><code>limiter_burst = TokenBucketRateLimiter(capacity=5, tokens_per_second=1)\nlimiter_long = TokenBucketRateLimiter(capacity=30, tokens_per_minute=10)\n\n@app.get(\"/multi-token-bucket\", dependencies=[Depends(limiter_burst), Depends(limiter_long)])\nasync def multi_token_bucket_limited():\n    return {\"message\": \"You passed both token bucket rate limits!\"}\n</code></pre>"},{"location":"strategies/token_bucket/#3-available-configuration-options","title":"3. Available Configuration Options","text":"<p>You can customize the Token Bucket limiter using the following parameters:</p> Parameter Type Description Default <code>capacity</code> <code>int</code> Required. Maximum number of tokens the bucket can hold (burst size). Must be positive. \u2014 <code>tokens_per_second</code> <code>float</code> Number of tokens added per second. <code>0</code> <code>tokens_per_minute</code> <code>float</code> Number of tokens added per minute. <code>0</code> <code>tokens_per_hour</code> <code>float</code> Number of tokens added per hour. <code>0</code> <code>tokens_per_day</code> <code>float</code> Number of tokens added per day. <code>0</code> <code>key_func</code> <code>Callable</code> Function to extract a unique key from the request. By default, uses client IP and path. <code>on_limit</code> <code>Callable</code> Function called when the rate limit is exceeded. By default, raises HTTP 429. <code>prefix</code> <code>str</code> Redis key prefix for all limiter keys. <code>\"cap\"</code> <p>Note: - The total refill rate is the sum of all <code>tokens_per_*</code> arguments, converted to tokens per second. - At least one refill rate must be positive, and <code>capacity</code> must be positive.</p> <p>Example:</p> <pre><code># 100 token burst, refilling at 10 tokens per minute, with a custom prefix\nlimiter = TokenBucketRateLimiter(capacity=100, tokens_per_minute=10, prefix=\"myapi\")\n</code></pre>"},{"location":"strategies/token_bucket/#4-how-token-bucket-works-with-example","title":"4. How Token Bucket Works (with Example)","text":"<p>Suppose you set a capacity of 10 tokens and a refill rate of 1 token per second.</p> <ul> <li>The bucket starts full (10 tokens).</li> <li>Each request consumes 1 token.</li> <li>If 10 requests arrive instantly, all are allowed (bucket is now empty).</li> <li>Further requests are denied until tokens are refilled (1 per second).</li> <li>After 5 seconds, 5 tokens are available again, allowing 5 more requests.</li> </ul> <p>Visualization:</p> Time Tokens in Bucket Request? Allowed? Reason 12:00:00 10 Yes \u2705 Bucket full 12:00:00 9 Yes \u2705 ... ... ... ... ... 12:00:00 1 Yes \u2705 12:00:00 0 Yes \u2705 Last token used 12:00:01 0 Yes \u274c No tokens left 12:00:01 1 (refilled) Yes \u2705 1 token refilled 12:00:02 1 (refilled) Yes \u2705 1 token refilled"},{"location":"strategies/token_bucket/#5-notes-pros-cons","title":"5. Notes, Pros &amp; Cons","text":"<p>Notes:</p> <ul> <li>This strategy is excellent for APIs that need to allow short bursts but enforce a steady average rate.</li> <li>The <code>retry_after</code> value is based on how long until the next token is available.</li> </ul> <p>Pros:</p> <ul> <li>Allows bursts up to the bucket capacity.</li> <li>Smooths out traffic over time.</li> <li>Flexible and widely used in real-world APIs.</li> </ul> <p>Cons:</p> <ul> <li>Slightly more complex than Fixed Window.</li> <li>If refill rate or capacity is misconfigured, can allow more requests than intended in short bursts.</li> </ul> <p>Use Token Bucket when you want to allow bursts but enforce a steady average rate over time.</p>"}]}